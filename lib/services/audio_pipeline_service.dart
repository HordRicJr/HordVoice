import 'dart:async';
import 'dart:math';
import 'package:flutter/material.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:flutter_tts/flutter_tts.dart';
import '../models/voice_models.dart';
import '../theme/design_tokens.dart';
import 'permission_manager_service.dart';
import 'voice_calibration_service.dart';
import 'azure_wake_word_service.dart';
import 'azure_speech_service.dart';
import 'azure_speech_phrase_hints_service.dart';
// Nouveaux services d'optimisation
import 'voice_performance_monitoring_service.dart';
import 'audio_buffer_optimization_service.dart';
import 'smart_wake_word_detection_service.dart';
import 'voice_memory_optimization_service.dart';
import 'azure_api_optimization_service.dart';
import 'audio_compression_service.dart';

/// Provider pour le service de pipeline audio
final audioPipelineProvider =
    StateNotifierProvider<AudioPipelineNotifier, AudioPipelineState>(
      (ref) => AudioPipelineNotifier(),
    );

/// √âtat du pipeline audio
enum AudioPipelineStatus { idle, listening, processing, speaking, error }

/// √âtat complet du pipeline audio
class AudioPipelineState {
  final AudioPipelineStatus status;
  final bool isWakeWordActive;
  final double currentVolume;
  final List<double> waveformData;
  final String? lastRecognizedText;
  final String? currentSpeech;
  final EmotionType currentEmotion;
  final String? error;
  final VoiceOption? selectedVoice;

  const AudioPipelineState({
    this.status = AudioPipelineStatus.idle,
    this.isWakeWordActive = false,
    this.currentVolume = 0.0,
    this.waveformData = const [],
    this.lastRecognizedText,
    this.currentSpeech,
    this.currentEmotion = EmotionType.neutral,
    this.error,
    this.selectedVoice,
  });

  AudioPipelineState copyWith({
    AudioPipelineStatus? status,
    bool? isWakeWordActive,
    double? currentVolume,
    List<double>? waveformData,
    String? lastRecognizedText,
    String? currentSpeech,
    EmotionType? currentEmotion,
    String? error,
    VoiceOption? selectedVoice,
  }) {
    return AudioPipelineState(
      status: status ?? this.status,
      isWakeWordActive: isWakeWordActive ?? this.isWakeWordActive,
      currentVolume: currentVolume ?? this.currentVolume,
      waveformData: waveformData ?? this.waveformData,
      lastRecognizedText: lastRecognizedText ?? this.lastRecognizedText,
      currentSpeech: currentSpeech ?? this.currentSpeech,
      currentEmotion: currentEmotion ?? this.currentEmotion,
      error: error ?? this.error,
      selectedVoice: selectedVoice ?? this.selectedVoice,
    );
  }
}

/// Notifier pour le pipeline audio
class AudioPipelineNotifier extends StateNotifier<AudioPipelineState> {
  AudioPipelineNotifier() : super(const AudioPipelineState()) {
    _initialize();
  }

  // Services existants
  final VoiceCalibrationService _calibrationService = VoiceCalibrationService();
  final AzureWakeWordService _wakeWordService = AzureWakeWordService();
  final AzureSpeechService _speechService = AzureSpeechService();
  late FlutterTts _tts;

  // Nouveaux services d'optimisation
  late VoicePerformanceMonitoringService _performanceService;
  late AudioBufferOptimizationService _bufferService;
  late SmartWakeWordDetectionService _smartWakeWordService;
  late VoiceMemoryOptimizationService _memoryService;
  late AzureApiOptimizationService _apiService;
  late AudioCompressionService _compressionService;

  // Timers et streams
  Timer? _wakeWordTimer;
  Timer? _volumeTimer;
  Timer? _listeningTimeout; // Timeout pour arr√™ter l'√©coute automatiquement
  StreamSubscription? _audioStreamSubscription;

  // Configuration
  static const int _waveformBars = 24;
  static const Duration _listeningTimeoutDuration = Duration(seconds: 8); // Timeout √©coute
  static const Duration _wakeWordDetectionInterval = Duration(seconds: 10); // Intervalle wake word

  /// Initialise le pipeline audio avec optimisations
  Future<void> _initialize() async {
    try {
      // Initialiser tous les services d'optimisation
      await _initializeOptimizationServices();

      // V√©rifier les permissions
      final hasPermissions = await PermissionManagerService.hasEssentialPermissions();
      if (!hasPermissions) {
        state = state.copyWith(
          status: AudioPipelineStatus.error,
          error: 'Permissions microphone requises',
        );
        return;
      }

      // Initialiser la calibration
      await _calibrationService.initialize();

      // Initialiser les services Azure
      await _wakeWordService.initialize();
      await _speechService.initialize();

      // Configurer les phrase hints pour une pr√©cision maximale
      await _configurePhraseHints();

      // Initialiser TTS
      _tts = FlutterTts();
      await _initializeTTS();

      // Charger la voix par d√©faut
      final defaultVoice = VoiceOption(
        id: 'clara',
        name: 'Clara',
        language: 'fr-FR',
        style: 'warm',
        gender: 'female',
        description: 'Voix f√©minine chaleureuse',
      );
      state = state.copyWith(
        status: AudioPipelineStatus.idle,
        selectedVoice: defaultVoice,
      );

      // D√©marrer la d√©tection intelligente du wake word
      _startSmartWakeWordDetection();
    } catch (e) {
      state = state.copyWith(
        status: AudioPipelineStatus.error,
        error: 'Erreur initialisation: $e',
      );
    }
  }

  /// Initialise tous les services d'optimisation
  Future<void> _initializeOptimizationServices() async {
    debugPrint('üöÄ Initialisation des services d\'optimisation vocale...');

    // Service de monitoring des performances
    _performanceService = VoicePerformanceMonitoringService();
    await _performanceService.initialize();
    await _performanceService.startMonitoring();

    // Service d'optimisation des buffers audio
    _bufferService = AudioBufferOptimizationService();
    await _bufferService.initialize();

    // Service de d√©tection intelligente de wake word
    _smartWakeWordService = SmartWakeWordDetectionService();
    await _smartWakeWordService.initialize();

    // Service d'optimisation m√©moire
    _memoryService = VoiceMemoryOptimizationService();
    await _memoryService.initialize();
    await _memoryService.startOptimization();

    // Service d'optimisation des API Azure
    _apiService = AzureApiOptimizationService();
    await _apiService.initialize();

    // Service de compression audio
    _compressionService = AudioCompressionService();
    await _compressionService.initialize();

    debugPrint('‚úÖ Services d\'optimisation initialis√©s avec succ√®s');
  }

  /// D√©marre la d√©tection intelligente du wake word
  void _startSmartWakeWordDetection() {
    state = state.copyWith(isWakeWordActive: true);

    // Utiliser le service de d√©tection intelligente
    _startSmartWakeWordService();
  }

  /// D√©marre le service de d√©tection intelligente
  Future<void> _startSmartWakeWordService() async {
    try {
      debugPrint('üß† D√©marrage d√©tection intelligente wake word...');

      // D√©marrer l'√©coute intelligente
      await _smartWakeWordService.startListening();

      // √âcouter les d√©tections intelligentes
      _smartWakeWordService.detectionStream.listen((smartResult) {
        if (smartResult.originalResult.isDetected &&
            !smartResult.originalResult.needsConfirmation &&
            state.status == AudioPipelineStatus.idle) {
          debugPrint(
            '‚úÖ Wake word intelligent d√©tect√©: ${smartResult.originalResult.matchedText} '
            '(conf: ${smartResult.adjustedConfidence.toStringAsFixed(2)}, '
            '√©nergie: ${smartResult.energyLevel.toStringAsFixed(2)})',
          );
          _onWakeWordDetected();
        }
      });

      // √âcouter les infos d'activit√© vocale
      _smartWakeWordService.vadStream.listen((vadInfo) {
        // Mettre √† jour l'√©tat d'activit√© vocale si n√©cessaire
        debugPrint('VAD: ${vadInfo.isActive ? "Activit√©" : "Silence"} d√©tect√©');
      });

    } catch (e) {
      debugPrint('Erreur d√©tection intelligente wake word: $e');
      // Fallback vers la d√©tection standard
      _fallbackToStandardWakeWord();
    }
  }

  /// Fallback vers la d√©tection standard en cas d'erreur
  void _fallbackToStandardWakeWord() {
    debugPrint('üîÑ Fallback vers d√©tection wake word standard');
    _startWakeWordDetection();
  }

  /// D√©marre la d√©tection du wake word (m√©thode originale)
  void _startWakeWordDetection() {
    state = state.copyWith(isWakeWordActive: true);

    // IMPL√âMENTATION: Vraie d√©tection wake word avec Azure
    _startRealWakeWordDetection();
  }

  /// IMPL√âMENTATION: Vraie d√©tection wake word avec Azure Speech
  Future<void> _startRealWakeWordDetection() async {
    try {
      debugPrint('D√©marrage d√©tection wake word Azure...');

      // D√©marrer l'√©coute continue pour wake word
      await _wakeWordService.startListening();

      // √âcouter les d√©tections
      _wakeWordService.detectionStream.listen((detection) {
        if (detection.isDetected &&
            !detection.needsConfirmation &&
            state.status == AudioPipelineStatus.idle) {
          debugPrint(
            'Wake word d√©tect√©: ${detection.matchedText} (${detection.confidence})',
          );
          _onWakeWordDetected();
        }
      });
    } catch (e) {
      debugPrint('Erreur d√©tection wake word Azure: $e');
      // Fallback vers simulation si Azure √©choue
      _fallbackToSimulatedWakeWord();
    }
  }

  /// Gestionnaire d'√©v√©nement wake word d√©tect√©
  void _onWakeWordDetected() {
    startListening();
  }

  /// Initialise le TTS
  Future<void> _initializeTTS() async {
    try {
      await _tts.setLanguage('fr-FR');
      await _tts.setSpeechRate(0.5);
      await _tts.setVolume(0.8);
      await _tts.setPitch(1.0);
      debugPrint('TTS initialis√© avec succ√®s');
    } catch (e) {
      debugPrint('Erreur initialisation TTS: $e');
    }
  }

  /// Configure les phrase hints pour optimiser la reconnaissance vocale
  Future<void> _configurePhraseHints() async {
    try {
      debugPrint('üéØ Configuration des phrase hints pour HordVoice...');

      // Configurer TOUTES les phrases pour une pr√©cision maximale
      final success = await AzureSpeechPhraseHintsService.configureAllHints();

      if (success) {
        final stats = AzureSpeechPhraseHintsService.getPhrasesStats();
        debugPrint(
          '‚úÖ Phrase hints configur√©es: ${stats["TOTAL"]} phrases au total',
        );
        debugPrint(
          'üìä R√©partition: Wake words: ${stats["wake_words"]}, Syst√®me: ${stats["system"]}, Navigation: ${stats["navigation"]}, M√©t√©o: ${stats["weather"]}, T√©l√©phonie: ${stats["telephony"]}, Musique: ${stats["music"]}, etc.',
        );
      } else {
        debugPrint('‚ùå √âchec de la configuration des phrase hints');
      }
    } catch (e) {
      debugPrint('üö® Erreur lors de la configuration des phrase hints: $e');
    }
  }

  /// Fallback vers simulation si Azure Wake Word √©choue
  void _fallbackToSimulatedWakeWord() {
    debugPrint('Fallback vers simulation wake word');
    _wakeWordTimer?.cancel();
    _wakeWordTimer = Timer.periodic(
      _wakeWordDetectionInterval,
      (_) => _simulateWakeWordDetection(),
    );
  }

  /// Simulation de d√©tection wake word
  void _simulateWakeWordDetection() {
    // Simulation al√©atoire pour demo
    if (Random().nextBool() && state.status == AudioPipelineStatus.idle) {
      debugPrint('Wake word d√©tect√© (simulation)');
      startListening();
    }
  }

  /// AJOUT: D√©marre le timeout d'√©coute pour √©viter l'√©coute infinie
  void _startListeningTimeout() {
    _listeningTimeout?.cancel();
    _listeningTimeout = Timer(_listeningTimeoutDuration, () {
      if (state.status == AudioPipelineStatus.listening) {
        debugPrint('Timeout d\'√©coute atteint - arr√™t automatique');
        stopListening();
      }
    });
  }

  /// D√©marre l'√©coute apr√®s wake word ou interaction tactile (optimis√©e)
  Future<void> startListening() async {
    if (state.status != AudioPipelineStatus.idle) return;

    final stopwatch = Stopwatch()..start();

    try {
      // Obtenir un contexte audio optimis√©
      final audioContext = _memoryService.getAudioContext();
      
      state = state.copyWith(
        status: AudioPipelineStatus.listening,
        currentEmotion: EmotionType.neutral,
        error: null,
      );

      // D√©marrer la g√©n√©ration de waveform optimis√©e
      _startOptimizedWaveformGeneration();

      // D√©marrer le timeout d'√©coute pour √©viter l'√©coute infinie
      _startListeningTimeout();

      // Reconnaissance vocale optimis√©e avec compression
      await _startOptimizedVoiceRecognition(audioContext);

      stopwatch.stop();

      // Enregistrer les m√©triques de performance
      _performanceService.recordVoiceRecognitionMetric(
        latency: stopwatch.elapsed,
        confidence: 0.0, // Will be updated when recognition completes
        audioDataSize: 0, // Will be updated with actual data
        recognizedText: 'listening_started',
      );

    } catch (e) {
      stopwatch.stop();
      debugPrint('Erreur d√©marrage √©coute optimis√©e: $e');
      
      // Fallback vers la m√©thode standard
      _startRealVoiceRecognition();
    }
  }

  /// D√©marre la reconnaissance vocale optimis√©e
  Future<void> _startOptimizedVoiceRecognition(dynamic audioContext) async {
    try {
      debugPrint('üé§ D√©marrage reconnaissance vocale optimis√©e...');

      // Allouer des buffers optimis√©s pour la reconnaissance
      final inputBuffer = _bufferService.allocateRecognitionBuffer();

      // D√©marrer la reconnaissance continue avec optimisations API
      await _speechService.startListening();

      // G√©rer le stream de r√©sultats avec optimisation m√©moire
      final subscription = _memoryService.manageStream(
        _speechService.resultStream,
        (result) => _handleOptimizedRecognitionResult(result, audioContext),
        streamId: 'voice_recognition',
        onError: (error) => _handleRecognitionError(error),
      );

      // Nettoyer le buffer apr√®s usage
      _scheduleBufferCleanup(inputBuffer);

    } catch (e) {
      debugPrint('Erreur reconnaissance optimis√©e: $e');
      // Fallback vers la m√©thode standard
      _startRealVoiceRecognition();
    }
  }

  /// Traite les r√©sultats de reconnaissance optimis√©s
  void _handleOptimizedRecognitionResult(dynamic result, dynamic audioContext) {
    if (result.recognizedText.isNotEmpty && state.status == AudioPipelineStatus.listening) {
      _listeningTimeout?.cancel(); // Arr√™ter timeout car commande reconnue

      debugPrint('‚úÖ Commande reconnue (optimis√©e): ${result.recognizedText}');

      state = state.copyWith(
        status: AudioPipelineStatus.processing,
        lastRecognizedText: result.recognizedText,
        currentEmotion: EmotionType.joy,
      );

      // Enregistrer les m√©triques de performance
      _performanceService.recordVoiceRecognitionMetric(
        latency: Duration(milliseconds: 50), // Temps depuis d√©but √©coute
        confidence: result.confidence,
        audioDataSize: 0, // Taille des donn√©es audio
        recognizedText: result.recognizedText,
      );

      // Traiter la commande de mani√®re optimis√©e
      _processOptimizedCommand(result.recognizedText, audioContext);
    }
  }

  /// Traite une commande de mani√®re optimis√©e
  Future<void> _processOptimizedCommand(String command, dynamic audioContext) async {
    try {
      // Retourner le contexte audio au pool
      _memoryService.returnAudioContext(audioContext);

      // Traitement existant de la commande
      await _processCommand(command);
      
    } catch (e) {
      debugPrint('Erreur traitement commande optimis√©e: $e');
      // Fallback vers le traitement standard
      await _processCommand(command);
    }
  }

  /// Programme le nettoyage d'un buffer
  void _scheduleBufferCleanup(dynamic buffer) {
    Timer(const Duration(seconds: 5), () {
      try {
        _bufferService.deallocateBuffer(buffer, context: 'recognition_cleanup');
      } catch (e) {
        debugPrint('Erreur nettoyage buffer: $e');
      }
    });
  }

  /// IMPL√âMENTATION: Vraie reconnaissance vocale avec Azure Speech
  Future<void> _startRealVoiceRecognition() async {
    try {
      debugPrint('D√©marrage reconnaissance vocale Azure...');

      // D√©marrer la reconnaissance continue
      await _speechService.startListening();

      // √âcouter les r√©sultats de transcription via stream
      _speechService.resultStream.listen((result) {
        if (result.recognizedText.isNotEmpty &&
            state.status == AudioPipelineStatus.listening) {
          _listeningTimeout?.cancel(); // Arr√™ter timeout car commande reconnue

          debugPrint('Commande reconnue: ${result.recognizedText}');

          state = state.copyWith(
            status: AudioPipelineStatus.processing,
            lastRecognizedText: result.recognizedText,
            currentEmotion: EmotionType.joy,
          );

          // Traiter la commande
          _processCommand(result.recognizedText);
        }
      });

      // G√©rer les erreurs de reconnaissance via stream
      _speechService.errorStream.listen((error) {
        debugPrint('Erreur reconnaissance vocale: ${error.errorMessage}');
        if (state.status == AudioPipelineStatus.listening) {
          // Fallback vers simulation si Azure √©choue
          _fallbackToSimulatedRecognition();
        }
      });
    } catch (e) {
      debugPrint('Erreur d√©marrage reconnaissance Azure: $e');
      // Fallback vers simulation
      _fallbackToSimulatedRecognition();
    }
  }

  /// Fallback vers simulation si Azure Speech √©choue
  void _fallbackToSimulatedRecognition() {
    debugPrint('Fallback vers simulation reconnaissance vocale');
    _simulateVoiceRecognition();
  }

  /// Stoppe l'√©coute
  void stopListening() {
    _volumeTimer?.cancel();
    _listeningTimeout?.cancel(); // AJOUT: Arr√™ter le timeout d'√©coute

    state = state.copyWith(
      status: AudioPipelineStatus.idle,
      currentVolume: 0.0,
      waveformData: List.filled(_waveformBars, 0.0),
    );
  }

  /// Simulation de reconnaissance vocale
  void _simulateVoiceRecognition() {
    Timer(const Duration(seconds: 3), () {
      // AJOUT: Arr√™ter le timeout d'√©coute car commande reconnue
      _listeningTimeout?.cancel();

      final commands = [
        'Quel temps fait-il ?',
        'Appelle maman',
        'Lis mes messages',
        'D√©marre la musique',
        'Navigation vers la maison',
      ];

      final recognizedText = commands[Random().nextInt(commands.length)];

      state = state.copyWith(
        status: AudioPipelineStatus.processing,
        lastRecognizedText: recognizedText,
        currentEmotion: EmotionType.joy,
      );

      // Traiter la commande
      _processCommand(recognizedText);
    });
  }

  /// Traite une commande reconnue
  Future<void> _processCommand(String command) async {
    // D√©terminer l'√©motion bas√©e sur la commande
    EmotionType emotion = EmotionType.neutral;
    String response = 'Commande re√ßue : $command';

    if (command.contains('temps') || command.contains('m√©t√©o')) {
      emotion = EmotionType.calm;
      response = 'Il fait beau aujourd\'hui, 22 degr√©s avec un ciel d√©gag√©.';
    } else if (command.contains('appelle') || command.contains('appel')) {
      emotion = EmotionType.joy;
      response = 'Je compose le num√©ro maintenant.';
    } else if (command.contains('musique')) {
      emotion = EmotionType.joy;
      response = 'Je lance ta playlist pr√©f√©r√©e.';
    } else if (command.contains('messages')) {
      emotion = EmotionType.neutral;
      response = 'Tu as 3 nouveaux messages. Veux-tu que je les lise ?';
    } else if (command.contains('navigation')) {
      emotion = EmotionType.calm;
      response = 'Calcul de l\'itin√©raire vers la maison en cours.';
    }

    // Commencer la synth√®se vocale
    await speak(response, emotion);
  }

  /// Synth√®se vocale avec √©motion
  Future<void> speak(String text, [EmotionType? emotion]) async {
    final startTime = DateTime.now();
    
    // Use memory-optimized synthesis context
    final synthContext = await _memoryOptimizationService.acquireSynthesisContext();
    
    try {
      state = state.copyWith(
        status: AudioPipelineStatus.speaking,
        currentSpeech: text,
        currentEmotion: emotion ?? EmotionType.neutral,
      );

      // Track synthesis performance
      await _performanceMonitoringService.recordMetric(
        'synthesis_start',
        0.0,
        {'text_length': text.length.toDouble()},
      );

      // IMPL√âMENTATION: Optimized speech synthesis with Azure Speech TTS
      await _speakWithAzure(text, emotion, synthContext);

      state = state.copyWith(
        status: AudioPipelineStatus.idle,
        currentSpeech: null,
        currentEmotion: EmotionType.neutral,
      );

      // Record synthesis completion metrics
      final latency = DateTime.now().difference(startTime).inMilliseconds.toDouble();
      await _performanceMonitoringService.recordMetric(
        'synthesis_latency',
        latency,
        {'text_length': text.length.toDouble()},
      );
    } finally {
      await _memoryOptimizationService.releaseSynthesisContext(synthContext);
    }
  }

  /// IMPL√âMENTATION: Optimized speech synthesis with FlutterTts and audio compression
  Future<void> _speakWithAzure(String text, EmotionType? emotion, dynamic synthContext) async {
    try {
      debugPrint('Synth√®se TTS optimis√©e: $text');

      // Optimize API call with Azure optimization service
      final optimizedRequest = await _azureApiOptimizationService.optimizeRequest(
        'synthesis',
        {'text': text, 'emotion': emotion?.toString()},
      );

      // Configure TTS voice according to selection and emotion
      if (state.selectedVoice != null) {
        await _configureTTSVoice(state.selectedVoice!, emotion);
      }

      // Get or create audio buffer for synthesis
      final audioBuffer = _audioBufferOptimizationService.createBuffer(1024 * 4); // 4KB for TTS output
      
      try {
        // Launch voice synthesis with compression
        await _tts.speak(text);
        
        // Compress audio output if available
        if (audioBuffer.isNotEmpty) {
          final compressedAudio = await _audioCompressionService.compressAudio(
            audioBuffer,
            quality: CompressionQuality.balanced,
          );
          
          // Log compression ratio for monitoring
          final compressionRatio = audioBuffer.length / compressedAudio.length;
          await _performanceMonitoringService.recordMetric(
            'synthesis_compression_ratio',
            compressionRatio,
            {'original_size': audioBuffer.length.toDouble()},
          );
        }
      } finally {
        _audioBufferOptimizationService.releaseBuffer(audioBuffer);
      }
    } catch (e) {
      debugPrint('Erreur synth√®se TTS optimis√©e: $e');
      
      // Record synthesis error
      await _performanceMonitoringService.recordMetric(
        'synthesis_error',
        1.0,
        {'error_type': e.runtimeType.toString()},
      );
      
      // Fallback to simulation
      await _simulateSpeech(text);
    }
  }

  /// Configure la voix TTS selon l'option s√©lectionn√©e et l'√©motion
  Future<void> _configureTTSVoice(
    VoiceOption voice,
    EmotionType? emotion,
  ) async {
    try {
      await _tts.setLanguage(voice.language);

      // Ajuster les param√®tres selon l'√©motion
      switch (emotion) {
        case EmotionType.joy:
          await _tts.setSpeechRate(0.6); // Plus rapide pour la joie
          await _tts.setPitch(1.2); // Plus aigu
          break;
        case EmotionType.calm:
          await _tts.setSpeechRate(0.4); // Plus lent pour le calme
          await _tts.setPitch(0.9); // L√©g√®rement plus grave
          break;
        case EmotionType.sadness:
          await _tts.setSpeechRate(0.3); // Tr√®s lent pour la tristesse
          await _tts.setPitch(0.8); // Plus grave
          break;
        default:
          await _tts.setSpeechRate(0.5); // Neutre
          await _tts.setPitch(1.0);
      }
    } catch (e) {
      debugPrint('Erreur configuration voix TTS: $e');
    }
  }

  /// Simulation de synth√®se vocale
  Future<void> _simulateSpeech(String text) async {
    final words = text.split(' ');
    final duration = Duration(
      milliseconds: words.length * 300,
    ); // ~300ms par mot

    await Future.delayed(duration);
  }

  /// Change la voix s√©lectionn√©e
  void selectVoice(VoiceOption voice) {
    state = state.copyWith(selectedVoice: voice);

    // IMPL√âMENTATION: Configurer le TTS avec la nouvelle voix
    _configureTTSWithNewVoice(voice);
    debugPrint('Voix chang√©e vers: ${voice.name}');
  }

  /// IMPL√âMENTATION: Configure le TTS avec une nouvelle voix
  Future<void> _configureTTSWithNewVoice(VoiceOption voice) async {
    try {
      // Configurer la langue
      await _tts.setLanguage(voice.language);

      // Configurer les param√®tres selon le style de voix
      switch (voice.style) {
        case 'warm':
          await _tts.setSpeechRate(0.5);
          await _tts.setPitch(1.1);
          await _tts.setVolume(0.8);
          break;
        case 'professional':
          await _tts.setSpeechRate(0.6);
          await _tts.setPitch(1.0);
          await _tts.setVolume(0.9);
          break;
        case 'friendly':
          await _tts.setSpeechRate(0.55);
          await _tts.setPitch(1.15);
          await _tts.setVolume(0.85);
          break;
        case 'calm':
          await _tts.setSpeechRate(0.4);
          await _tts.setPitch(0.9);
          await _tts.setVolume(0.7);
          break;
        default:
          await _tts.setSpeechRate(0.5);
          await _tts.setPitch(1.0);
          await _tts.setVolume(0.8);
      }

      // Ajuster selon le genre si sp√©cifi√©
      if (voice.gender == 'female') {
        // Pitch plus aigu pour voix f√©minine (d√©j√† configur√© dans le switch)
        debugPrint('Voix f√©minine configur√©e avec pitch plus aigu');
      } else if (voice.gender == 'male') {
        // Pitch plus grave pour voix masculine
        debugPrint('Voix masculine configur√©e avec pitch plus grave');
      }

      debugPrint(
        'TTS configur√© pour la voix: ${voice.name} (${voice.style}, ${voice.gender})',
      );
    } catch (e) {
      debugPrint('Erreur configuration TTS pour la voix ${voice.name}: $e');
    }
  }

  /// G√©n√®re les donn√©es de waveform optimis√©es
  void _startOptimizedWaveformGeneration() {
    _volumeTimer?.cancel();
    
    // Utiliser un buffer r√©utilisable pour les donn√©es waveform
    final waveformBuffer = _memoryService.getDoubleList();
    
    _volumeTimer = Timer.periodic(
      const Duration(milliseconds: 50), // 20 FPS
      (_) => _updateOptimizedWaveform(waveformBuffer),
    );
  }

  /// Met √† jour les donn√©es de waveform de mani√®re optimis√©e
  void _updateOptimizedWaveform(List<double> waveformBuffer) {
    if (state.status != AudioPipelineStatus.listening) return;

    try {
      // Simulation du niveau audio avec patterns r√©alistes et optimis√©s
      final random = Random();
      final baseLevel = 0.3 + random.nextDouble() * 0.4; // 0.3-0.7
      final spike = random.nextDouble() < 0.1 ? random.nextDouble() * 0.3 : 0.0;
      final currentVolume = (baseLevel + spike).clamp(0.0, 1.0);

      // R√©utiliser le buffer existant plut√¥t que cr√©er une nouvelle liste
      waveformBuffer.clear();
      
      // G√©n√©rer donn√©es waveform optimis√©es
      for (int index = 0; index < _waveformBars; index++) {
        final variation = random.nextDouble() * 0.3 - 0.15; // -0.15 √† +0.15
        final barLevel = (currentVolume + variation).clamp(0.0, 1.0);

        // Appliquer un pattern pour rendre plus naturel
        final pattern = sin((index / _waveformBars) * 2 * pi) * 0.1;
        waveformBuffer.add((barLevel + pattern).clamp(0.0, 1.0));
      }

      state = state.copyWith(
        currentVolume: currentVolume,
        waveformData: List.from(waveformBuffer), // Copie pour l'immutabilit√©
      );

    } catch (e) {
      debugPrint('Erreur waveform optimis√©e: $e');
      // Fallback vers la m√©thode standard
      _updateWaveform();
    }
  }

  /// G√©n√®re les donn√©es de waveform en temps r√©el
  void _startWaveformGeneration() {
    _volumeTimer?.cancel();
    _volumeTimer = Timer.periodic(
      const Duration(milliseconds: 50), // 20 FPS
      (_) => _updateWaveform(),
    );
  }

  /// Met √† jour les donn√©es de waveform
  void _updateWaveform() {
    if (state.status != AudioPipelineStatus.listening) return;

    // Simulation du niveau audio avec patterns r√©alistes
    final random = Random();
    final baseLevel = 0.3 + random.nextDouble() * 0.4; // 0.3-0.7
    final spike = random.nextDouble() < 0.1 ? random.nextDouble() * 0.3 : 0.0;
    final currentVolume = (baseLevel + spike).clamp(0.0, 1.0);

    // G√©n√©rer donn√©es waveform
    final waveformData = List.generate(_waveformBars, (index) {
      final variation = random.nextDouble() * 0.3 - 0.15; // -0.15 √† +0.15
      final barLevel = (currentVolume + variation).clamp(0.0, 1.0);

      // Appliquer un pattern pour rendre plus naturel
      final pattern = sin((index / _waveformBars) * 2 * pi) * 0.1;
      return (barLevel + pattern).clamp(0.0, 1.0);
    });

    state = state.copyWith(
      currentVolume: currentVolume,
      waveformData: waveformData,
    );
  }

  /// G√®re l'erreur de permission
  void handlePermissionError() {
    state = state.copyWith(
      status: AudioPipelineStatus.error,
      error: 'Permissions requises pour utiliser le microphone',
    );
  }

  /// R√©initialise apr√®s erreur
  void resetFromError() {
    state = state.copyWith(status: AudioPipelineStatus.idle, error: null);
    _startWakeWordDetection();
  }

  /// Active/d√©sactive le wake word
  void toggleWakeWord(bool enabled) {
    if (enabled) {
      _startWakeWordDetection();
    } else {
      _wakeWordTimer?.cancel();
      state = state.copyWith(isWakeWordActive: false);
    }
  }

  /// Configure dynamiquement les phrase hints selon le contexte d'usage
  Future<void> configurePhraseHintsForContext(String context) async {
    try {
      debugPrint('üéØ Configuration phrase hints pour contexte: $context');

      bool success = false;

      switch (context.toLowerCase()) {
        case 'wake_word':
          success =
              await AzureSpeechPhraseHintsService.configureWakeWordHints();
          break;
        case 'navigation':
          success =
              await AzureSpeechPhraseHintsService.configureNavigationHints();
          break;
        case 'weather':
          success = await AzureSpeechPhraseHintsService.configureWeatherHints();
          break;
        case 'music':
          success = await AzureSpeechPhraseHintsService.configureMusicHints();
          break;
        case 'telephony':
          success =
              await AzureSpeechPhraseHintsService.configureTelephonyHints();
          break;
        case 'messaging':
          success =
              await AzureSpeechPhraseHintsService.configureMessagingHints();
          break;
        case 'calendar':
          success =
              await AzureSpeechPhraseHintsService.configureCalendarHints();
          break;
        case 'health':
          success = await AzureSpeechPhraseHintsService.configureHealthHints();
          break;
        case 'emergency':
          success =
              await AzureSpeechPhraseHintsService.configureEmergencyHints();
          break;
        case 'all':
        case 'complete':
        default:
          success = await AzureSpeechPhraseHintsService.configureAllHints();
          break;
      }

      if (success) {
        debugPrint('‚úÖ Phrase hints pour "$context" configur√©es avec succ√®s');
      } else {
        debugPrint('‚ùå √âchec configuration phrase hints pour "$context"');
      }
    } catch (e) {
      debugPrint('üö® Erreur configuration phrase hints pour "$context": $e');
    }
  }

  /// Efface toutes les phrase hints configur√©es
  Future<void> clearPhraseHints() async {
    try {
      debugPrint('üßπ Effacement de toutes les phrase hints...');

      final success = await AzureSpeechPhraseHintsService.clearAllHints();

      if (success) {
        debugPrint('‚úÖ Phrase hints effac√©es avec succ√®s');
      } else {
        debugPrint('‚ùå √âchec de l\'effacement des phrase hints');
      }
    } catch (e) {
      debugPrint('üö® Erreur lors de l\'effacement des phrase hints: $e');
    }
  }

  /// Obtient le niveau de calibration
  double getCalibrationQuality() {
    return _calibrationService.getCalibrationQuality();
  }

  /// V√©rifie si une recalibration est n√©cessaire
  bool shouldRecalibrate() {
    return _calibrationService.shouldRecalibrate();
  }

  /// Am√©liore le profil avec de nouveaux √©chantillons
  Future<void> improveProfile(String recognizedText, double confidence) async {
    await _calibrationService.improveProfile(recognizedText, confidence);
  }

  /// AJOUT: Configure les timeouts dynamiquement
  void configureTimeouts({
    Duration? listeningTimeout,
    Duration? wakeWordInterval,
  }) {
    // Note: Pour une impl√©mentation compl√®te, on ajouterait des variables
    // d'instance pour stocker ces valeurs configurables
    debugPrint('Configuration des timeouts:');
    debugPrint(
      '  - Timeout √©coute: ${listeningTimeout ?? _listeningTimeoutDuration}',
    );
    debugPrint(
      '  - Intervalle wake word: ${wakeWordInterval ?? _wakeWordDetectionInterval}',
    );
  }

  /// AJOUT: Force l'arr√™t de tous les timers
  void stopAllTimers() {
    _wakeWordTimer?.cancel();
    _volumeTimer?.cancel();
    _listeningTimeout?.cancel();
    debugPrint('Tous les timers arr√™t√©s');
  }

  @override
  void dispose() {
    _wakeWordTimer?.cancel();
    _volumeTimer?.cancel();
    _listeningTimeout?.cancel(); // AJOUT: Nettoyer le timeout d'√©coute
    _audioStreamSubscription?.cancel();
    _calibrationService.dispose();
    
    // Dispose optimization services
    _performanceMonitoringService.dispose();
    _audioBufferOptimizationService.dispose();
    _smartWakeWordDetectionService.dispose();
    _memoryOptimizationService.dispose();
    _azureApiOptimizationService.dispose();
    _audioCompressionService.dispose();
    
    super.dispose();
  }
}

/// Extensions pour l'√©tat du pipeline
extension AudioPipelineStateExtensions on AudioPipelineState {
  /// V√©rifie si le pipeline est actif
  bool get isActive =>
      status != AudioPipelineStatus.idle && status != AudioPipelineStatus.error;

  /// V√©rifie si l'audio est en cours
  bool get isAudioActive =>
      status == AudioPipelineStatus.listening ||
      status == AudioPipelineStatus.speaking;

  /// Obtient la couleur de l'√©motion actuelle
  Color get emotionColor => currentEmotion.primaryColor;

  /// Obtient le gradient de l'√©motion actuelle
  Gradient get emotionGradient => currentEmotion.gradient;

  /// V√©rifie si une erreur critique est pr√©sente
  bool get hasCriticalError =>
      status == AudioPipelineStatus.error && error != null;

  /// Obtient le message d'√©tat utilisateur
  String get statusMessage {
    switch (status) {
      case AudioPipelineStatus.idle:
        return isWakeWordActive
            ? 'Dis "Hey Ric" ou touche-moi'
            : 'Assistant en veille';
      case AudioPipelineStatus.listening:
        return 'Je t\'√©coute...';
      case AudioPipelineStatus.processing:
        return 'Traitement en cours...';
      case AudioPipelineStatus.speaking:
        return 'Je r√©ponds...';
      case AudioPipelineStatus.error:
        return error ?? 'Erreur inconnue';
    }
  }
}

/// Commandes vocales pr√©d√©finies
class VoiceCommands {
  static const Map<String, List<String>> commands = {
    'weather': ['m√©t√©o', 'temps', 'temp√©rature', 'climat'],
    'call': ['appelle', 'compose', 't√©l√©phone', 'contact'],
    'music': ['musique', 'chanson', 'playlist', 'audio'],
    'messages': ['messages', 'SMS', 'textos', 'notifications'],
    'navigation': ['navigation', 'route', 'directions', 'aller'],
    'time': ['heure', 'temps', 'horloge'],
    'reminder': ['rappel', 'rappelle', 'note', 'm√©mo'],
    'help': ['aide', 'assistance', 'support', 'comment'],
  };

  /// D√©tecte le type de commande
  static String? detectCommandType(String text) {
    final normalizedText = text.toLowerCase();

    for (final entry in commands.entries) {
      for (final keyword in entry.value) {
        if (normalizedText.contains(keyword)) {
          return entry.key;
        }
      }
    }

    return null;
  }
}
