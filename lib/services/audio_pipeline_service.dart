import 'dart:async';
import 'dart:math';
import 'package:flutter/material.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:flutter_tts/flutter_tts.dart';
import '../models/voice_models.dart';
import '../theme/design_tokens.dart';
import 'permission_manager_service.dart';
import 'voice_calibration_service.dart';
import 'azure_wake_word_service.dart';
import 'azure_speech_service.dart';
import 'azure_speech_phrase_hints_service.dart';

/// Provider pour le service de pipeline audio
final audioPipelineProvider =
    StateNotifierProvider<AudioPipelineNotifier, AudioPipelineState>(
      (ref) => AudioPipelineNotifier(),
    );

/// √âtat du pipeline audio
enum AudioPipelineStatus { idle, listening, processing, speaking, error }

/// √âtat complet du pipeline audio
class AudioPipelineState {
  final AudioPipelineStatus status;
  final bool isWakeWordActive;
  final double currentVolume;
  final List<double> waveformData;
  final String? lastRecognizedText;
  final String? currentSpeech;
  final EmotionType currentEmotion;
  final String? error;
  final VoiceOption? selectedVoice;

  const AudioPipelineState({
    this.status = AudioPipelineStatus.idle,
    this.isWakeWordActive = false,
    this.currentVolume = 0.0,
    this.waveformData = const [],
    this.lastRecognizedText,
    this.currentSpeech,
    this.currentEmotion = EmotionType.neutral,
    this.error,
    this.selectedVoice,
  });

  AudioPipelineState copyWith({
    AudioPipelineStatus? status,
    bool? isWakeWordActive,
    double? currentVolume,
    List<double>? waveformData,
    String? lastRecognizedText,
    String? currentSpeech,
    EmotionType? currentEmotion,
    String? error,
    VoiceOption? selectedVoice,
  }) {
    return AudioPipelineState(
      status: status ?? this.status,
      isWakeWordActive: isWakeWordActive ?? this.isWakeWordActive,
      currentVolume: currentVolume ?? this.currentVolume,
      waveformData: waveformData ?? this.waveformData,
      lastRecognizedText: lastRecognizedText ?? this.lastRecognizedText,
      currentSpeech: currentSpeech ?? this.currentSpeech,
      currentEmotion: currentEmotion ?? this.currentEmotion,
      error: error ?? this.error,
      selectedVoice: selectedVoice ?? this.selectedVoice,
    );
  }
}

/// Notifier pour le pipeline audio
class AudioPipelineNotifier extends StateNotifier<AudioPipelineState> {
  AudioPipelineNotifier() : super(const AudioPipelineState()) {
    _initialize();
  }

  // Services
  final VoiceCalibrationService _calibrationService = VoiceCalibrationService();
  final AzureWakeWordService _wakeWordService = AzureWakeWordService();
  final AzureSpeechService _speechService = AzureSpeechService();
  late FlutterTts _tts;

  // Timers et streams
  Timer? _wakeWordTimer;
  Timer? _volumeTimer;
  Timer?
  _listeningTimeout; // AJOUT: Timeout pour arr√™ter l'√©coute automatiquement
  StreamSubscription? _audioStreamSubscription;

  // Configuration
  static const int _waveformBars = 24;
  static const Duration _listeningTimeoutDuration = Duration(
    seconds: 8,
  ); // Timeout √©coute
  static const Duration _wakeWordDetectionInterval = Duration(
    seconds: 10,
  ); // Intervalle wake word

  /// Initialise le pipeline audio
  Future<void> _initialize() async {
    try {
      // V√©rifier les permissions
      final hasPermissions =
          await PermissionManagerService.hasEssentialPermissions();
      if (!hasPermissions) {
        state = state.copyWith(
          status: AudioPipelineStatus.error,
          error: 'Permissions microphone requises',
        );
        return;
      }

      // Initialiser la calibration
      await _calibrationService.initialize();

      // Initialiser les services Azure
      await _wakeWordService.initialize();
      await _speechService.initialize();

      // Configurer les phrase hints pour une pr√©cision maximale
      await _configurePhraseHints();

      // Initialiser TTS
      _tts = FlutterTts();
      await _initializeTTS();

      // Charger la voix par d√©faut
      final defaultVoice = VoiceOption(
        id: 'clara',
        name: 'Clara',
        language: 'fr-FR',
        style: 'warm',
        gender: 'female',
        description: 'Voix f√©minine chaleureuse',
      );
      state = state.copyWith(
        status: AudioPipelineStatus.idle,
        selectedVoice: defaultVoice,
      );

      // D√©marrer la d√©tection du wake word
      _startWakeWordDetection();
    } catch (e) {
      state = state.copyWith(
        status: AudioPipelineStatus.error,
        error: 'Erreur initialisation: $e',
      );
    }
  }

  /// D√©marre la d√©tection du wake word
  void _startWakeWordDetection() {
    state = state.copyWith(isWakeWordActive: true);

    // IMPL√âMENTATION: Vraie d√©tection wake word avec Azure
    _startRealWakeWordDetection();
  }

  /// IMPL√âMENTATION: Vraie d√©tection wake word avec Azure Speech
  Future<void> _startRealWakeWordDetection() async {
    try {
      debugPrint('D√©marrage d√©tection wake word Azure...');

      // D√©marrer l'√©coute continue pour wake word
      await _wakeWordService.startListening();

      // √âcouter les d√©tections
      _wakeWordService.detectionStream.listen((detection) {
        if (detection.isDetected &&
            !detection.needsConfirmation &&
            state.status == AudioPipelineStatus.idle) {
          debugPrint(
            'Wake word d√©tect√©: ${detection.matchedText} (${detection.confidence})',
          );
          _onWakeWordDetected();
        }
      });
    } catch (e) {
      debugPrint('Erreur d√©tection wake word Azure: $e');
      // Fallback vers simulation si Azure √©choue
      _fallbackToSimulatedWakeWord();
    }
  }

  /// Gestionnaire d'√©v√©nement wake word d√©tect√©
  void _onWakeWordDetected() {
    startListening();
  }

  /// Initialise le TTS
  Future<void> _initializeTTS() async {
    try {
      await _tts.setLanguage('fr-FR');
      await _tts.setSpeechRate(0.5);
      await _tts.setVolume(0.8);
      await _tts.setPitch(1.0);
      debugPrint('TTS initialis√© avec succ√®s');
    } catch (e) {
      debugPrint('Erreur initialisation TTS: $e');
    }
  }

  /// Configure les phrase hints pour optimiser la reconnaissance vocale
  Future<void> _configurePhraseHints() async {
    try {
      debugPrint('üéØ Configuration des phrase hints pour HordVoice...');

      // Configurer TOUTES les phrases pour une pr√©cision maximale
      final success = await AzureSpeechPhraseHintsService.configureAllHints();

      if (success) {
        final stats = AzureSpeechPhraseHintsService.getPhrasesStats();
        debugPrint(
          '‚úÖ Phrase hints configur√©es: ${stats["TOTAL"]} phrases au total',
        );
        debugPrint(
          'üìä R√©partition: Wake words: ${stats["wake_words"]}, Syst√®me: ${stats["system"]}, Navigation: ${stats["navigation"]}, M√©t√©o: ${stats["weather"]}, T√©l√©phonie: ${stats["telephony"]}, Musique: ${stats["music"]}, etc.',
        );
      } else {
        debugPrint('‚ùå √âchec de la configuration des phrase hints');
      }
    } catch (e) {
      debugPrint('üö® Erreur lors de la configuration des phrase hints: $e');
    }
  }

  /// Fallback vers simulation si Azure Wake Word √©choue
  void _fallbackToSimulatedWakeWord() {
    debugPrint('Fallback vers simulation wake word');
    _wakeWordTimer?.cancel();
    _wakeWordTimer = Timer.periodic(
      _wakeWordDetectionInterval,
      (_) => _simulateWakeWordDetection(),
    );
  }

  /// Simulation de d√©tection wake word
  void _simulateWakeWordDetection() {
    // Simulation al√©atoire pour demo
    if (Random().nextBool() && state.status == AudioPipelineStatus.idle) {
      debugPrint('Wake word d√©tect√© (simulation)');
      startListening();
    }
  }

  /// AJOUT: D√©marre le timeout d'√©coute pour √©viter l'√©coute infinie
  void _startListeningTimeout() {
    _listeningTimeout?.cancel();
    _listeningTimeout = Timer(_listeningTimeoutDuration, () {
      if (state.status == AudioPipelineStatus.listening) {
        debugPrint('Timeout d\'√©coute atteint - arr√™t automatique');
        stopListening();
      }
    });
  }

  /// D√©marre l'√©coute apr√®s wake word ou interaction tactile
  Future<void> startListening() async {
    if (state.status != AudioPipelineStatus.idle) return;

    state = state.copyWith(
      status: AudioPipelineStatus.listening,
      currentEmotion: EmotionType.neutral,
      error: null,
    );

    // D√©marrer la g√©n√©ration de waveform
    _startWaveformGeneration();

    // AJOUT: D√©marrer le timeout d'√©coute pour √©viter l'√©coute infinie
    _startListeningTimeout();

    // IMPL√âMENTATION: Vraie reconnaissance vocale avec Azure Speech
    _startRealVoiceRecognition();
  }

  /// IMPL√âMENTATION: Vraie reconnaissance vocale avec Azure Speech
  Future<void> _startRealVoiceRecognition() async {
    try {
      debugPrint('D√©marrage reconnaissance vocale Azure...');

      // D√©marrer la reconnaissance continue
      await _speechService.startListening();

      // √âcouter les r√©sultats de transcription via stream
      _speechService.resultStream.listen((result) {
        if (result.recognizedText.isNotEmpty &&
            state.status == AudioPipelineStatus.listening) {
          _listeningTimeout?.cancel(); // Arr√™ter timeout car commande reconnue

          debugPrint('Commande reconnue: ${result.recognizedText}');

          state = state.copyWith(
            status: AudioPipelineStatus.processing,
            lastRecognizedText: result.recognizedText,
            currentEmotion: EmotionType.joy,
          );

          // Traiter la commande
          _processCommand(result.recognizedText);
        }
      });

      // G√©rer les erreurs de reconnaissance via stream
      _speechService.errorStream.listen((error) {
        debugPrint('Erreur reconnaissance vocale: ${error.errorMessage}');
        if (state.status == AudioPipelineStatus.listening) {
          // Fallback vers simulation si Azure √©choue
          _fallbackToSimulatedRecognition();
        }
      });
    } catch (e) {
      debugPrint('Erreur d√©marrage reconnaissance Azure: $e');
      // Fallback vers simulation
      _fallbackToSimulatedRecognition();
    }
  }

  /// Fallback vers simulation si Azure Speech √©choue
  void _fallbackToSimulatedRecognition() {
    debugPrint('Fallback vers simulation reconnaissance vocale');
    _simulateVoiceRecognition();
  }

  /// Stoppe l'√©coute
  void stopListening() {
    _volumeTimer?.cancel();
    _listeningTimeout?.cancel(); // AJOUT: Arr√™ter le timeout d'√©coute

    state = state.copyWith(
      status: AudioPipelineStatus.idle,
      currentVolume: 0.0,
      waveformData: List.filled(_waveformBars, 0.0),
    );
  }

  /// Simulation de reconnaissance vocale
  void _simulateVoiceRecognition() {
    Timer(const Duration(seconds: 3), () {
      // AJOUT: Arr√™ter le timeout d'√©coute car commande reconnue
      _listeningTimeout?.cancel();

      final commands = [
        'Quel temps fait-il ?',
        'Appelle maman',
        'Lis mes messages',
        'D√©marre la musique',
        'Navigation vers la maison',
      ];

      final recognizedText = commands[Random().nextInt(commands.length)];

      state = state.copyWith(
        status: AudioPipelineStatus.processing,
        lastRecognizedText: recognizedText,
        currentEmotion: EmotionType.joy,
      );

      // Traiter la commande
      _processCommand(recognizedText);
    });
  }

  /// Traite une commande reconnue
  Future<void> _processCommand(String command) async {
    // D√©terminer l'√©motion bas√©e sur la commande
    EmotionType emotion = EmotionType.neutral;
    String response = 'Commande re√ßue : $command';

    if (command.contains('temps') || command.contains('m√©t√©o')) {
      emotion = EmotionType.calm;
      response = 'Il fait beau aujourd\'hui, 22 degr√©s avec un ciel d√©gag√©.';
    } else if (command.contains('appelle') || command.contains('appel')) {
      emotion = EmotionType.joy;
      response = 'Je compose le num√©ro maintenant.';
    } else if (command.contains('musique')) {
      emotion = EmotionType.joy;
      response = 'Je lance ta playlist pr√©f√©r√©e.';
    } else if (command.contains('messages')) {
      emotion = EmotionType.neutral;
      response = 'Tu as 3 nouveaux messages. Veux-tu que je les lise ?';
    } else if (command.contains('navigation')) {
      emotion = EmotionType.calm;
      response = 'Calcul de l\'itin√©raire vers la maison en cours.';
    }

    // Commencer la synth√®se vocale
    await speak(response, emotion);
  }

  /// Synth√®se vocale avec √©motion
  Future<void> speak(String text, [EmotionType? emotion]) async {
    state = state.copyWith(
      status: AudioPipelineStatus.speaking,
      currentSpeech: text,
      currentEmotion: emotion ?? EmotionType.neutral,
    );

    // IMPL√âMENTATION: Vraie synth√®se vocale avec Azure Speech TTS
    await _speakWithAzure(text, emotion);

    state = state.copyWith(
      status: AudioPipelineStatus.idle,
      currentSpeech: null,
      currentEmotion: EmotionType.neutral,
    );
  }

  /// IMPL√âMENTATION: Synth√®se vocale avec FlutterTts et configuration voix
  Future<void> _speakWithAzure(String text, EmotionType? emotion) async {
    try {
      debugPrint('Synth√®se TTS: $text');

      // Configurer la voix selon la s√©lection et l'√©motion
      if (state.selectedVoice != null) {
        await _configureTTSVoice(state.selectedVoice!, emotion);
      }

      // Lancer la synth√®se vocale
      await _tts.speak(text);
    } catch (e) {
      debugPrint('Erreur synth√®se TTS: $e');
      // Fallback vers simulation
      await _simulateSpeech(text);
    }
  }

  /// Configure la voix TTS selon l'option s√©lectionn√©e et l'√©motion
  Future<void> _configureTTSVoice(
    VoiceOption voice,
    EmotionType? emotion,
  ) async {
    try {
      await _tts.setLanguage(voice.language);

      // Ajuster les param√®tres selon l'√©motion
      switch (emotion) {
        case EmotionType.joy:
          await _tts.setSpeechRate(0.6); // Plus rapide pour la joie
          await _tts.setPitch(1.2); // Plus aigu
          break;
        case EmotionType.calm:
          await _tts.setSpeechRate(0.4); // Plus lent pour le calme
          await _tts.setPitch(0.9); // L√©g√®rement plus grave
          break;
        case EmotionType.sadness:
          await _tts.setSpeechRate(0.3); // Tr√®s lent pour la tristesse
          await _tts.setPitch(0.8); // Plus grave
          break;
        default:
          await _tts.setSpeechRate(0.5); // Neutre
          await _tts.setPitch(1.0);
      }
    } catch (e) {
      debugPrint('Erreur configuration voix TTS: $e');
    }
  }

  /// Simulation de synth√®se vocale
  Future<void> _simulateSpeech(String text) async {
    final words = text.split(' ');
    final duration = Duration(
      milliseconds: words.length * 300,
    ); // ~300ms par mot

    await Future.delayed(duration);
  }

  /// Change la voix s√©lectionn√©e
  void selectVoice(VoiceOption voice) {
    state = state.copyWith(selectedVoice: voice);

    // IMPL√âMENTATION: Configurer le TTS avec la nouvelle voix
    _configureTTSWithNewVoice(voice);
    debugPrint('Voix chang√©e vers: ${voice.name}');
  }

  /// IMPL√âMENTATION: Configure le TTS avec une nouvelle voix
  Future<void> _configureTTSWithNewVoice(VoiceOption voice) async {
    try {
      // Configurer la langue
      await _tts.setLanguage(voice.language);

      // Configurer les param√®tres selon le style de voix
      switch (voice.style) {
        case 'warm':
          await _tts.setSpeechRate(0.5);
          await _tts.setPitch(1.1);
          await _tts.setVolume(0.8);
          break;
        case 'professional':
          await _tts.setSpeechRate(0.6);
          await _tts.setPitch(1.0);
          await _tts.setVolume(0.9);
          break;
        case 'friendly':
          await _tts.setSpeechRate(0.55);
          await _tts.setPitch(1.15);
          await _tts.setVolume(0.85);
          break;
        case 'calm':
          await _tts.setSpeechRate(0.4);
          await _tts.setPitch(0.9);
          await _tts.setVolume(0.7);
          break;
        default:
          await _tts.setSpeechRate(0.5);
          await _tts.setPitch(1.0);
          await _tts.setVolume(0.8);
      }

      // Ajuster selon le genre si sp√©cifi√©
      if (voice.gender == 'female') {
        // Pitch plus aigu pour voix f√©minine (d√©j√† configur√© dans le switch)
        debugPrint('Voix f√©minine configur√©e avec pitch plus aigu');
      } else if (voice.gender == 'male') {
        // Pitch plus grave pour voix masculine
        debugPrint('Voix masculine configur√©e avec pitch plus grave');
      }

      debugPrint(
        'TTS configur√© pour la voix: ${voice.name} (${voice.style}, ${voice.gender})',
      );
    } catch (e) {
      debugPrint('Erreur configuration TTS pour la voix ${voice.name}: $e');
    }
  }

  /// G√©n√®re les donn√©es de waveform en temps r√©el
  void _startWaveformGeneration() {
    _volumeTimer?.cancel();
    _volumeTimer = Timer.periodic(
      const Duration(milliseconds: 50), // 20 FPS
      (_) => _updateWaveform(),
    );
  }

  /// Met √† jour les donn√©es de waveform
  void _updateWaveform() {
    if (state.status != AudioPipelineStatus.listening) return;

    // Simulation du niveau audio avec patterns r√©alistes
    final random = Random();
    final baseLevel = 0.3 + random.nextDouble() * 0.4; // 0.3-0.7
    final spike = random.nextDouble() < 0.1 ? random.nextDouble() * 0.3 : 0.0;
    final currentVolume = (baseLevel + spike).clamp(0.0, 1.0);

    // G√©n√©rer donn√©es waveform
    final waveformData = List.generate(_waveformBars, (index) {
      final variation = random.nextDouble() * 0.3 - 0.15; // -0.15 √† +0.15
      final barLevel = (currentVolume + variation).clamp(0.0, 1.0);

      // Appliquer un pattern pour rendre plus naturel
      final pattern = sin((index / _waveformBars) * 2 * pi) * 0.1;
      return (barLevel + pattern).clamp(0.0, 1.0);
    });

    state = state.copyWith(
      currentVolume: currentVolume,
      waveformData: waveformData,
    );
  }

  /// G√®re l'erreur de permission
  void handlePermissionError() {
    state = state.copyWith(
      status: AudioPipelineStatus.error,
      error: 'Permissions requises pour utiliser le microphone',
    );
  }

  /// R√©initialise apr√®s erreur
  void resetFromError() {
    state = state.copyWith(status: AudioPipelineStatus.idle, error: null);
    _startWakeWordDetection();
  }

  /// Active/d√©sactive le wake word
  void toggleWakeWord(bool enabled) {
    if (enabled) {
      _startWakeWordDetection();
    } else {
      _wakeWordTimer?.cancel();
      state = state.copyWith(isWakeWordActive: false);
    }
  }

  /// Configure dynamiquement les phrase hints selon le contexte d'usage
  Future<void> configurePhraseHintsForContext(String context) async {
    try {
      debugPrint('üéØ Configuration phrase hints pour contexte: $context');

      bool success = false;

      switch (context.toLowerCase()) {
        case 'wake_word':
          success =
              await AzureSpeechPhraseHintsService.configureWakeWordHints();
          break;
        case 'navigation':
          success =
              await AzureSpeechPhraseHintsService.configureNavigationHints();
          break;
        case 'weather':
          success = await AzureSpeechPhraseHintsService.configureWeatherHints();
          break;
        case 'music':
          success = await AzureSpeechPhraseHintsService.configureMusicHints();
          break;
        case 'telephony':
          success =
              await AzureSpeechPhraseHintsService.configureTelephonyHints();
          break;
        case 'messaging':
          success =
              await AzureSpeechPhraseHintsService.configureMessagingHints();
          break;
        case 'calendar':
          success =
              await AzureSpeechPhraseHintsService.configureCalendarHints();
          break;
        case 'health':
          success = await AzureSpeechPhraseHintsService.configureHealthHints();
          break;
        case 'emergency':
          success =
              await AzureSpeechPhraseHintsService.configureEmergencyHints();
          break;
        case 'all':
        case 'complete':
        default:
          success = await AzureSpeechPhraseHintsService.configureAllHints();
          break;
      }

      if (success) {
        debugPrint('‚úÖ Phrase hints pour "$context" configur√©es avec succ√®s');
      } else {
        debugPrint('‚ùå √âchec configuration phrase hints pour "$context"');
      }
    } catch (e) {
      debugPrint('üö® Erreur configuration phrase hints pour "$context": $e');
    }
  }

  /// Efface toutes les phrase hints configur√©es
  Future<void> clearPhraseHints() async {
    try {
      debugPrint('üßπ Effacement de toutes les phrase hints...');

      final success = await AzureSpeechPhraseHintsService.clearAllHints();

      if (success) {
        debugPrint('‚úÖ Phrase hints effac√©es avec succ√®s');
      } else {
        debugPrint('‚ùå √âchec de l\'effacement des phrase hints');
      }
    } catch (e) {
      debugPrint('üö® Erreur lors de l\'effacement des phrase hints: $e');
    }
  }

  /// Obtient le niveau de calibration
  double getCalibrationQuality() {
    return _calibrationService.getCalibrationQuality();
  }

  /// V√©rifie si une recalibration est n√©cessaire
  bool shouldRecalibrate() {
    return _calibrationService.shouldRecalibrate();
  }

  /// Am√©liore le profil avec de nouveaux √©chantillons
  Future<void> improveProfile(String recognizedText, double confidence) async {
    await _calibrationService.improveProfile(recognizedText, confidence);
  }

  /// AJOUT: Configure les timeouts dynamiquement
  void configureTimeouts({
    Duration? listeningTimeout,
    Duration? wakeWordInterval,
  }) {
    // Note: Pour une impl√©mentation compl√®te, on ajouterait des variables
    // d'instance pour stocker ces valeurs configurables
    debugPrint('Configuration des timeouts:');
    debugPrint(
      '  - Timeout √©coute: ${listeningTimeout ?? _listeningTimeoutDuration}',
    );
    debugPrint(
      '  - Intervalle wake word: ${wakeWordInterval ?? _wakeWordDetectionInterval}',
    );
  }

  /// AJOUT: Force l'arr√™t de tous les timers
  void stopAllTimers() {
    _wakeWordTimer?.cancel();
    _volumeTimer?.cancel();
    _listeningTimeout?.cancel();
    debugPrint('Tous les timers arr√™t√©s');
  }

  @override
  void dispose() {
    _wakeWordTimer?.cancel();
    _volumeTimer?.cancel();
    _listeningTimeout?.cancel(); // AJOUT: Nettoyer le timeout d'√©coute
    _audioStreamSubscription?.cancel();
    _calibrationService.dispose();
    super.dispose();
  }
}

/// Extensions pour l'√©tat du pipeline
extension AudioPipelineStateExtensions on AudioPipelineState {
  /// V√©rifie si le pipeline est actif
  bool get isActive =>
      status != AudioPipelineStatus.idle && status != AudioPipelineStatus.error;

  /// V√©rifie si l'audio est en cours
  bool get isAudioActive =>
      status == AudioPipelineStatus.listening ||
      status == AudioPipelineStatus.speaking;

  /// Obtient la couleur de l'√©motion actuelle
  Color get emotionColor => currentEmotion.primaryColor;

  /// Obtient le gradient de l'√©motion actuelle
  Gradient get emotionGradient => currentEmotion.gradient;

  /// V√©rifie si une erreur critique est pr√©sente
  bool get hasCriticalError =>
      status == AudioPipelineStatus.error && error != null;

  /// Obtient le message d'√©tat utilisateur
  String get statusMessage {
    switch (status) {
      case AudioPipelineStatus.idle:
        return isWakeWordActive
            ? 'Dis "Hey Ric" ou touche-moi'
            : 'Assistant en veille';
      case AudioPipelineStatus.listening:
        return 'Je t\'√©coute...';
      case AudioPipelineStatus.processing:
        return 'Traitement en cours...';
      case AudioPipelineStatus.speaking:
        return 'Je r√©ponds...';
      case AudioPipelineStatus.error:
        return error ?? 'Erreur inconnue';
    }
  }
}

/// Commandes vocales pr√©d√©finies
class VoiceCommands {
  static const Map<String, List<String>> commands = {
    'weather': ['m√©t√©o', 'temps', 'temp√©rature', 'climat'],
    'call': ['appelle', 'compose', 't√©l√©phone', 'contact'],
    'music': ['musique', 'chanson', 'playlist', 'audio'],
    'messages': ['messages', 'SMS', 'textos', 'notifications'],
    'navigation': ['navigation', 'route', 'directions', 'aller'],
    'time': ['heure', 'temps', 'horloge'],
    'reminder': ['rappel', 'rappelle', 'note', 'm√©mo'],
    'help': ['aide', 'assistance', 'support', 'comment'],
  };

  /// D√©tecte le type de commande
  static String? detectCommandType(String text) {
    final normalizedText = text.toLowerCase();

    for (final entry in commands.entries) {
      for (final keyword in entry.value) {
        if (normalizedText.contains(keyword)) {
          return entry.key;
        }
      }
    }

    return null;
  }
}
