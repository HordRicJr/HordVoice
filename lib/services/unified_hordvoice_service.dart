import 'dart:async';
import 'dart:math';
import 'package:flutter/foundation.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:device_info_plus/device_info_plus.dart';
import 'package:battery_plus/battery_plus.dart';
import 'package:geolocator/geolocator.dart';
import 'package:flutter_tts/flutter_tts.dart';
import 'package:supabase_flutter/supabase_flutter.dart';
import 'package:shared_preferences/shared_preferences.dart';
import 'package:flutter_local_notifications/flutter_local_notifications.dart';

import '../models/user_profile.dart';
import '../models/ai_models.dart';
import 'azure_openai_service.dart';
import 'azure_speech_service.dart';
import 'emotion_analysis_service.dart';
import 'weather_service.dart';
import 'news_service.dart';
import 'spotify_service.dart';
import 'navigation_service.dart';
import 'calendar_service.dart';
import 'health_monitoring_service.dart';
import 'phone_monitoring_service.dart';
import 'battery_monitoring_service.dart';
import 'voice_management_service.dart'; // √âtape 11: Gestion des voix
import 'quick_settings_service.dart'; // Contr√¥les vocaux syst√®me
import 'transition_animation_service.dart'; // Transitions avatar

final unifiedHordVoiceServiceProvider = Provider<UnifiedHordVoiceService>((
  ref,
) {
  return UnifiedHordVoiceService();
});

class UnifiedHordVoiceService {
  static final UnifiedHordVoiceService _instance =
      UnifiedHordVoiceService._internal();
  factory UnifiedHordVoiceService() => _instance;
  UnifiedHordVoiceService._internal();

  // Modification: Supabase peut √™tre null si pas encore initialis√©
  SupabaseClient? _supabase;
  late FlutterTts _tts;
  late Battery _battery;
  late FlutterLocalNotificationsPlugin _notifications;

  UserProfile? _currentUser;
  Timer? _monitoringTimer;
  Timer? _batteryTimer;
  Timer? _wakeWordTimer;
  bool _isInitialized = false;
  bool _isInitializing = false;
  bool _secondaryInitialized = false;
  bool _isListening = false;
  bool _wakeWordActive = false;
  String _currentMood = 'neutral';

  late AzureOpenAIService _aiService;
  late AzureSpeechService _azureSpeechService;
  late EmotionAnalysisService _emotionAnalysisService;
  late WeatherService _weatherService;
  late NewsService _newsService;
  late SpotifyService _spotifyService;
  late NavigationService _navigationService;
  late CalendarService _calendarService;
  late HealthMonitoringService _healthService;
  late PhoneMonitoringService _phoneMonitoringService;
  late BatteryMonitoringService _batteryMonitoringService;
  late VoiceManagementService
  _voiceManagementService; // √âtape 11: Gestion des voix
  late QuickSettingsService _quickSettingsService; // Contr√¥les vocaux syst√®me
  late TransitionAnimationService _transitionService; // Transitions avatar

  final StreamController<String> _aiResponseController =
      StreamController<String>.broadcast();
  final StreamController<String> _moodController =
      StreamController<String>.broadcast();
  final StreamController<Map<String, dynamic>> _systemStatusController =
      StreamController<Map<String, dynamic>>.broadcast();

  // Nouveaux streams pour pipeline audio (√âtape 6)
  final StreamController<double> _audioLevelController =
      StreamController<double>.broadcast();
  final StreamController<String> _transcriptionController =
      StreamController<String>.broadcast();
  final StreamController<Map<String, dynamic>> _emotionController =
      StreamController<Map<String, dynamic>>.broadcast();
  final StreamController<bool> _wakeWordController =
      StreamController<bool>.broadcast();
  final StreamController<bool> _isSpeakingController =
      StreamController<bool>.broadcast();

  Stream<String> get aiResponseStream => _aiResponseController.stream;
  Stream<String> get moodStream => _moodController.stream;
  Stream<Map<String, dynamic>> get systemStatusStream =>
      _systemStatusController.stream;

  // Getters pour nouveaux streams (√âtape 6)
  Stream<double> get audioLevelStream => _audioLevelController.stream;
  Stream<String> get transcriptionStream => _transcriptionController.stream;
  Stream<Map<String, dynamic>> get emotionStream => _emotionController.stream;
  Stream<bool> get wakeWordStream => _wakeWordController.stream;
  Stream<bool> get isSpeakingStream => _isSpeakingController.stream;

  /// Initialisation progressive - Services essentiels
  Future<void> initializeCore() async {
    if (_isInitialized) {
      debugPrint('UnifiedHordVoiceService d√©j√† initialis√© - Ignorer');
      return;
    }

    // Protection contre les appels multiples simultan√©s
    if (_isInitializing) {
      debugPrint('Initialisation en cours - Attendre');
      return;
    }

    _isInitializing = true;

    try {
      debugPrint('üöÄ D√âBUT Initialisation UnifiedHordVoiceService');

      // V√©rifier que Supabase est initialis√©
      try {
        _supabase = Supabase.instance.client;
      } catch (e) {
        throw Exception(
          'Supabase doit √™tre initialis√© avant UnifiedHordVoiceService: $e',
        );
      }

      _tts = FlutterTts();
      _battery = Battery();
      _notifications = FlutterLocalNotificationsPlugin();

      await _initializeNotifications();
      await _initializeTTS();

      // Initialiser seulement les services essentiels
      _aiService = AzureOpenAIService();
      _azureSpeechService = AzureSpeechService();
      _emotionAnalysisService = EmotionAnalysisService();

      try {
        await _aiService.initialize();
      } catch (e) {
        debugPrint('Avertissement: AzureOpenAI non disponible: $e');
      }

      try {
        await _azureSpeechService.initialize();
      } catch (e) {
        debugPrint('Avertissement: AzureSpeech non disponible: $e');
      }

      try {
        await _emotionAnalysisService.initialize();
      } catch (e) {
        debugPrint('Avertissement: EmotionAnalysis non disponible: $e');
      }

      debugPrint('Services essentiels initialis√©s');
      _isInitialized = true;
      debugPrint('‚úÖ FIN Initialisation UnifiedHordVoiceService - SUCC√àS');
    } catch (e) {
      debugPrint(
        'Erreur lors de l\'initialisation des services essentiels: $e',
      );
      throw e;
    } finally {
      _isInitializing = false;
    }
  }

  /// Initialisation progressive - Services secondaires
  Future<void> initializeSecondary() async {
    if (_secondaryInitialized) {
      debugPrint('Services secondaires d√©j√† initialis√©s - Ignorer');
      return;
    }

    // Marquer imm√©diatement pour √©viter les appels multiples
    _secondaryInitialized = true;

    try {
      debugPrint('üîß D√âBUT Initialisation services secondaires');

      // Initialiser les services secondaires
      _weatherService = WeatherService();
      _newsService = NewsService();
      _spotifyService = SpotifyService();
      _navigationService = NavigationService();
      _calendarService = CalendarService();
      _healthService = HealthMonitoringService();
      _phoneMonitoringService = PhoneMonitoringService();
      _batteryMonitoringService = BatteryMonitoringService();
      _voiceManagementService = VoiceManagementService();
      _quickSettingsService = QuickSettingsService();
      _transitionService = TransitionAnimationService();

      // Initialisation safe avec gestion d'erreurs
      final services = [
        () => _weatherService.initialize(),
        () => _newsService.initialize(),
        () => _spotifyService.initialize(),
        () => _navigationService.initialize(),
        () => _calendarService.initialize(),
        () => _healthService.initialize(),
        () => _phoneMonitoringService.initialize(),
        () => _batteryMonitoringService.initialize(),
        () => _voiceManagementService.initialize(),
        () => _quickSettingsService.initialize(),
        () => _transitionService.initialize(),
      ];

      for (final serviceInitializer in services) {
        try {
          await serviceInitializer();
        } catch (e) {
          debugPrint('Avertissement: Un service secondaire a √©chou√©: $e');
          // Continuer avec les autres services
        }
      }

      _isInitialized = true;

      // D√©marrer les services de monitoring
      try {
        await _startSystemMonitoring();
        await _startWakeWordDetection();
      } catch (e) {
        debugPrint('Avertissement: Monitoring non disponible: $e');
      }

      debugPrint('Tous les services initialis√©s');
    } catch (e) {
      debugPrint(
        'Erreur lors de l\'initialisation des services secondaires: $e',
      );
      // En cas d'erreur, on garde _secondaryInitialized = true pour √©viter les boucles
      debugPrint(
        'Services secondaires marqu√©s comme initialis√©s malgr√© l\'erreur',
      );
    }
  }

  Future<void> initialize() async {
    // M√©thode compl√®te pour compatibilit√©
    // √âviter la double initialisation
    if (_isInitialized) {
      debugPrint(
        'UnifiedHordVoiceService d√©j√† compl√®tement initialis√© - Ignorer',
      );
      return;
    }

    await initializeCore();
    await initializeSecondary();
  }

  Future<void> _initializeNotifications() async {
    const androidSettings = AndroidInitializationSettings(
      '@mipmap/ic_launcher',
    );
    const iosSettings = DarwinInitializationSettings();
    const initSettings = InitializationSettings(
      android: androidSettings,
      iOS: iosSettings,
    );

    await _notifications.initialize(initSettings);
  }

  Future<void> _initializeTTS() async {
    await _tts.setLanguage('fr-FR');
    await _tts.setSpeechRate(0.8);
    await _tts.setVolume(0.8);
    await _tts.setPitch(1.0);
  }

  Future<UserProfile?> getCurrentUser() async {
    if (_currentUser != null) return _currentUser;

    try {
      if (_supabase == null) {
        debugPrint('Supabase non initialis√©');
        return null;
      }

      final prefs = await SharedPreferences.getInstance();
      final deviceId = prefs.getString('device_id');

      if (deviceId == null) return null;

      final response = await _supabase!
          .from('user_profiles')
          .select()
          .eq('device_id', deviceId)
          .single();

      _currentUser = UserProfile.fromJson(response);
      return _currentUser;
    } catch (e) {
      debugPrint('Erreur lors de la r√©cup√©ration de l\'utilisateur: $e');
      return null;
    }
  }

  Future<UserProfile> createUser({
    required String firstName,
    String? lastName,
    String? nickname,
    String personalityType = 'mere_africaine',
    String relationshipMode = 'ami',
  }) async {
    try {
      if (_supabase == null) {
        throw Exception(
          'Supabase non initialis√© pour la cr√©ation d\'utilisateur',
        );
      }

      final deviceInfo = DeviceInfoPlugin();
      String deviceId;

      if (defaultTargetPlatform == TargetPlatform.android) {
        final androidInfo = await deviceInfo.androidInfo;
        deviceId = androidInfo.id;
      } else {
        deviceId = DateTime.now().millisecondsSinceEpoch.toString();
      }

      final userData = {
        'device_id': deviceId,
        'first_name': firstName,
        'last_name': lastName,
        'nickname': nickname,
        'personality_preference': personalityType,
        'relationship_mode': relationshipMode,
        'created_at': DateTime.now().toIso8601String(),
        'updated_at': DateTime.now().toIso8601String(),
        'last_active': DateTime.now().toIso8601String(),
      };

      final response = await _supabase!
          .from('user_profiles')
          .insert(userData)
          .select()
          .single();

      _currentUser = UserProfile.fromJson(response);

      final prefs = await SharedPreferences.getInstance();
      await prefs.setString('device_id', deviceId);

      await _speakWelcomeMessage();

      return _currentUser!;
    } catch (e) {
      debugPrint('Erreur lors de la cr√©ation de l\'utilisateur: $e');
      rethrow;
    }
  }

  Future<void> _speakWelcomeMessage() async {
    if (_currentUser == null) return;

    final responses = await _getPersonalityResponses(
      'felicitation',
      'premiere_utilisation',
    );
    if (responses.isNotEmpty) {
      final welcomeMessage = responses.first.responseText;
      await speakText(welcomeMessage);
    }
  }

  Future<void> speakText(String text, {String? emotion}) async {
    try {
      // √âtape 6: Signaler qu'on commence √† parler
      _isSpeakingController.add(true);

      if (_currentUser != null) {
        await _tts.setSpeechRate(_currentUser!.speechSpeed);
        await _tts.setVolume(_currentUser!.volume);
      }

      await _tts.speak(text);
      _aiResponseController.add(text);

      // Signaler qu'on a fini de parler
      _isSpeakingController.add(false);
    } catch (e) {
      _isSpeakingController.add(false);
      debugPrint('Erreur lors de la synth√®se vocale: $e');
    }
  }

  /// √âtape 6: Interrompre TTS si l'utilisateur parle
  Future<void> interruptSpeech() async {
    try {
      await _tts.stop();
      _isSpeakingController.add(false);
      debugPrint('TTS interrompu par l\'utilisateur');
    } catch (e) {
      debugPrint('Erreur lors de l\'interruption TTS: $e');
    }
  }

  Future<void> startListening() async {
    if (_isListening) return;

    _isListening = true;

    // √âtape 6: Pipeline audio streaming avec Azure Speech
    await _azureSpeechService.startListening();

    // √âcouter les r√©sultats de transcription en streaming
    _azureSpeechService.resultStream.listen((result) {
      _transcriptionController.add(result.recognizedText);
      if (result.isFinal) {
        _processVoiceCommandWithEmotion(result.recognizedText);
      }
    });

    // √âcouter les erreurs
    _azureSpeechService.errorStream.listen((error) {
      debugPrint('Erreur STT: ${error.errorMessage}');
      speakText('D√©sol√©, je n\'ai pas bien entendu.');
    });
  }

  Future<void> stopListening() async {
    if (!_isListening) return;

    _isListening = false;
    await _azureSpeechService.stopListening();
  }

  /// √âtape 6: Pipeline audio avec analyse parall√®le (texte + √©motion)
  Future<void> _processVoiceCommandWithEmotion(String command) async {
    try {
      // Parall√©lisation: text analytics + emotion analysis
      final Future<String> emotionFuture = _emotionAnalysisService
          .analyzeEmotion(command);
      final Future<String> intentFuture = _aiService.analyzeIntent(command);

      // Attendre les deux analyses en parall√®le
      final results = await Future.wait([emotionFuture, intentFuture]);
      final emotion = results[0];
      final intent = results[1];

      // Publier l'√©motion dans le stream
      _emotionController.add({
        'emotion': emotion,
        'text': command,
        'timestamp': DateTime.now().toIso8601String(),
        'confidence': 0.85, // TODO: r√©cup√©rer vraie confidence
      });

      // Mettre √† jour l'humeur et ex√©cuter l'intention
      await _updateMood(emotion);
      await _executeIntent(intent, command);
    } catch (e) {
      debugPrint('Erreur lors du traitement avec √©motion: $e');
      await speakText('D√©sol√©, je n\'ai pas compris votre demande.');
    }
  }

  Future<void> _executeIntent(String intent, String originalCommand) async {
    switch (intent.toLowerCase()) {
      case 'weather':
        await _handleWeatherRequest(originalCommand);
        break;
      case 'news':
        await _handleNewsRequest(originalCommand);
        break;
      case 'music':
        await _handleMusicRequest(originalCommand);
        break;
      case 'navigation':
        await _handleNavigationRequest(originalCommand);
        break;
      case 'calendar':
        await _handleCalendarRequest(originalCommand);
        break;
      case 'health':
        await _handleHealthRequest(originalCommand);
        break;
      case 'system':
        await _handleSystemRequest(originalCommand);
        break;
      // √âtape 11: Gestion des voix voice-only
      case 'voice':
      case 'voix':
        await _handleVoiceRequest(originalCommand);
        break;
      default:
        await _handleGeneralConversation(originalCommand);
    }
  }

  Future<void> _handleWeatherRequest(String command) async {
    try {
      final position = await Geolocator.getCurrentPosition();
      final weather = await _weatherService.getCurrentWeather(
        position.latitude,
        position.longitude,
      );

      final response = await _aiService.generateContextualResponse(
        'Donne-moi la m√©t√©o',
        'weather',
        {'weather': weather},
      );
      await speakText(response);
    } catch (e) {
      await speakText(
        'Je ne peux pas acc√©der aux informations m√©t√©o pour le moment.',
      );
    }
  }

  Future<void> _handleNewsRequest(String command) async {
    try {
      final news = await _newsService.getLatestNews();
      final response = await _aiService.generateContextualResponse(
        command,
        'news',
        {'news': news},
      );
      await speakText(response);
    } catch (e) {
      await speakText('Je ne peux pas acc√©der aux actualit√©s pour le moment.');
    }
  }

  Future<void> _handleMusicRequest(String command) async {
    try {
      if (command.toLowerCase().contains('spotify')) {
        await _spotifyService.playMusic(command);
      } else {
        await _spotifyService.searchAndPlay(command);
      }
      await speakText('Je lance votre musique !');
    } catch (e) {
      await speakText('Je ne peux pas lancer la musique pour le moment.');
    }
  }

  Future<void> _handleNavigationRequest(String command) async {
    try {
      final lowerCommand = command.toLowerCase();

      // √âtape 8: Recherche POI voice-only
      final poiKeywords = [
        'trouve',
        'cherche',
        'restaurant',
        'pharmacie',
        'magasin',
        'h√¥pital',
        'banque',
      ];
      final hasPOISearch = poiKeywords.any(
        (keyword) => lowerCommand.contains(keyword),
      );

      if (hasPOISearch) {
        // Extraire le type de POI recherch√©
        String poiQuery = '';
        if (lowerCommand.contains('restaurant'))
          poiQuery = 'restaurant';
        else if (lowerCommand.contains('pharmacie'))
          poiQuery = 'pharmacie';
        else if (lowerCommand.contains('magasin'))
          poiQuery = 'magasin';
        else if (lowerCommand.contains('h√¥pital'))
          poiQuery = 'h√¥pital';
        else if (lowerCommand.contains('banque'))
          poiQuery = 'banque';
        else {
          // Fallback: extraire mot-cl√© du texte
          final words = lowerCommand.split(' ');
          poiQuery = words.firstWhere(
            (word) =>
                word.length > 3 &&
                !['trouve', 'cherche', 'pr√®s', 'dans'].contains(word),
            orElse: () => 'lieu',
          );
        }

        // Demander permission si n√©cessaire
        if (!await _navigationService.isLocationAvailable()) {
          final permissionResponse = await _navigationService
              .requestLocationPermissionVoiceOnly();
          await speakText(permissionResponse);
          if (permissionResponse.contains('refus√©e') ||
              permissionResponse.contains('bloqu√©e')) {
            return;
          }
        }

        // Rechercher POI
        final searchResult = await _navigationService.searchPOIVoiceOnly(
          poiQuery,
        );
        final voiceResponse = _navigationService.generateVoiceResponseForPOI(
          searchResult,
        );
        await speakText(voiceResponse);
        return;
      }

      // Navigation classique vers destination
      // Utiliser Azure OpenAI pour extraire la destination
      final extractResponse = await _aiService.generatePersonalizedResponse(
        'Extrait uniquement le nom de la destination de cette phrase: $command',
        'navigation_assistant',
        _currentUser?.id ?? 'anonymous',
        [],
      );

      if (extractResponse.isNotEmpty && extractResponse.length > 3) {
        try {
          final directions = await _navigationService.getDirections(
            extractResponse,
          );
          final response = await _aiService.generateContextualResponse(
            command,
            'navigation',
            {'directions': directions, 'destination': extractResponse},
          );
          await speakText(response);
        } catch (e) {
          await speakText(
            'Je ne peux pas calculer l\'itin√©raire vers $extractResponse.',
          );
        }
      } else {
        await speakText(
          'Je n\'ai pas compris la destination. Pouvez-vous r√©p√©ter ?',
        );
      }
    } catch (e) {
      debugPrint('Erreur navigation: $e');
      await speakText('Je ne peux pas d√©marrer la navigation pour le moment.');
    }
  }

  Future<void> _handleCalendarRequest(String command) async {
    try {
      final events = await _calendarService.getTodayEvents();
      final eventsMap = events
          .map(
            (event) => {
              'title': event.title,
              'description': event.description,
              'start': event.start?.toIso8601String(),
              'end': event.end?.toIso8601String(),
              'location': event.location,
            },
          )
          .toList();

      final response = await _aiService.generateContextualResponse(
        command,
        'calendar',
        {'events': eventsMap},
      );
      await speakText(response);
    } catch (e) {
      await speakText(
        'Je ne peux pas acc√©der √† votre calendrier pour le moment.',
      );
    }
  }

  Future<void> _handleHealthRequest(String command) async {
    try {
      final healthData = await _healthService.getHealthSummary();
      final response = await _aiService.generateContextualResponse(
        command,
        'health',
        {'healthData': healthData},
      );
      await speakText(response);
    } catch (e) {
      await speakText(
        'Je ne peux pas acc√©der √† vos donn√©es de sant√© pour le moment.',
      );
    }
  }

  Future<void> _handleSystemRequest(String command) async {
    try {
      final systemInfo = await _getSystemStatus();
      final response = await _aiService.generateContextualResponse(
        command,
        'system',
        systemInfo,
      );
      await speakText(response);
    } catch (e) {
      await speakText(
        'Je ne peux pas acc√©der aux informations syst√®me pour le moment.',
      );
    }
  }

  Future<void> _handleGeneralConversation(String command) async {
    try {
      final response = await _aiService.generatePersonalizedResponse(
        command,
        'assistant_vocal',
        _currentUser?.id ?? 'anonymous',
        [],
      );
      await speakText(response);
    } catch (e) {
      await speakText('Je ne comprends pas bien. Pouvez-vous reformuler ?');
    }
  }

  /// √âtape 11: Orchestration compl√®te de la gestion des voix
  Future<void> _handleVoiceRequest(String command) async {
    try {
      final lowerCommand = command.toLowerCase();

      // Lister les voix disponibles
      if (lowerCommand.contains(
        RegExp(r'\b(quelles? voix|liste voix|voix disponibles?)\b'),
      )) {
        final response = _voiceManagementService.generateVoiceListResponse();
        await speakText(response);
        return;
      }

      // S√©lectionner une voix par nom
      final chooseMatch = RegExp(
        r'\b(?:choisis|s√©lectionne|utilise)\s+(\w+)\b',
      ).firstMatch(lowerCommand);
      if (chooseMatch != null) {
        final voiceName = chooseMatch.group(1)!;
        final response = await _voiceManagementService.selectVoiceByName(
          voiceName,
        );

        // Aper√ßu dans la nouvelle voix si s√©lection r√©ussie
        if (!response.contains('D√©sol√©')) {
          await speakText(response);
          // TODO: Changer la voix TTS ici pour l'aper√ßu
          final selectedVoice = _voiceManagementService.selectedVoice;
          if (selectedVoice != null) {
            final preview = await _voiceManagementService.generateVoicePreview(
              selectedVoice,
            );
            await speakText(preview);
          }
        } else {
          await speakText(response);
        }
        return;
      }

      // Rafra√Æchir les voix
      if (lowerCommand.contains(
        RegExp(r'\b(actualise|rafra√Æchis|recharge)\s+(les\s+)?voix\b'),
      )) {
        await _voiceManagementService.refreshVoices();
        await speakText(
          'Liste des voix mise √† jour. Dites "quelles voix" pour entendre les nouvelles options.',
        );
        return;
      }

      // Commande vocale non reconnue
      await speakText(
        'Pour les voix, dites "quelles voix" pour la liste ou "choisis" suivi du nom.',
      );
    } catch (e) {
      debugPrint('Erreur gestion voix: $e');
      await speakText(
        'Probl√®me avec la gestion des voix. R√©essayez plus tard.',
      );
    }
  }

  Future<void> _updateMood(String emotion) async {
    _currentMood = emotion;
    _moodController.add(emotion);

    if (_currentUser != null && _supabase != null) {
      try {
        await _supabase!.from('emotion_logs').insert({
          'user_id': _currentUser!.id,
          'emotion_type': emotion,
          'detection_method': 'voice_analysis',
          'created_at': DateTime.now().toIso8601String(),
        });
      } catch (e) {
        debugPrint('Erreur lors de l\'enregistrement de l\'√©motion: $e');
      }
    }
  }

  Future<void> _startSystemMonitoring() async {
    _monitoringTimer = Timer.periodic(const Duration(minutes: 5), (
      timer,
    ) async {
      await _checkPhoneUsage();
      await _checkSystemHealth();
    });

    _batteryTimer = Timer.periodic(const Duration(minutes: 1), (timer) async {
      await _checkBatteryStatus();
    });
  }

  Future<void> _checkPhoneUsage() async {
    try {
      final usage = await _phoneMonitoringService.getCurrentUsage();
      if (usage.isExcessiveUsage && _currentUser?.allowReproches == true) {
        await _sendUsageWarning(usage);
      }
    } catch (e) {
      debugPrint('Erreur lors de la v√©rification de l\'utilisation: $e');
    }
  }

  Future<void> _checkBatteryStatus() async {
    try {
      final batteryLevel = await _battery.batteryLevel;
      final batteryState = await _battery.batteryState;

      if (batteryLevel <= 15 && batteryState != BatteryState.charging) {
        await _sendBatteryWarning(batteryLevel);
      }

      final batteryInfo = await _batteryMonitoringService
          .getCurrentBatteryHealth();
      if (batteryInfo.batteryTemperatureCelsius != null &&
          batteryInfo.batteryTemperatureCelsius! > 45.0) {
        await _sendOverheatingWarning(batteryInfo.batteryTemperatureCelsius!);
      }
    } catch (e) {
      debugPrint('Erreur lors de la v√©rification de la batterie: $e');
    }
  }

  Future<void> _sendUsageWarning(PhoneUsageMonitoring usage) async {
    final responses = await _getPersonalityResponses(
      'reproches',
      'usage_excessif',
    );
    if (responses.isNotEmpty) {
      final warning = responses[Random().nextInt(responses.length)];
      await speakText(warning.responseText);
      await _showNotification(
        'Attention √† votre utilisation !',
        'Vous avez d√©j√† pass√© ${(usage.totalScreenTimeSeconds / 3600).toStringAsFixed(1)} heures sur votre t√©l√©phone aujourd\'hui.',
      );
    }
  }

  Future<void> _sendBatteryWarning(int batteryLevel) async {
    final responses = await _getPersonalityResponses(
      'reproches',
      'batterie_faible',
    );
    if (responses.isNotEmpty) {
      final warning = responses[Random().nextInt(responses.length)];
      await speakText(warning.responseText);
      await _showNotification(
        'Batterie faible !',
        'Il ne reste que $batteryLevel% de batterie. Pensez √† recharger votre t√©l√©phone.',
      );
    }
  }

  Future<void> _sendOverheatingWarning(double temperature) async {
    final responses = await _getPersonalityResponses('reproches', 'surchauffe');
    if (responses.isNotEmpty) {
      final warning = responses[Random().nextInt(responses.length)];
      await speakText(warning.responseText);
      await _showNotification(
        'T√©l√©phone surchauff√© !',
        'Votre t√©l√©phone est √† ${temperature.toStringAsFixed(1)}¬∞C. Laissez-le refroidir.',
      );
    }
  }

  Future<List<AIPersonalityResponse>> _getPersonalityResponses(
    String responseType,
    String context,
  ) async {
    try {
      if (_supabase == null) {
        debugPrint('Supabase non initialis√© pour les r√©ponses de personnalit√©');
        return [];
      }

      final response = await _supabase!
          .from('ai_personality_responses')
          .select()
          .eq('response_type', responseType)
          .eq('trigger_context', context)
          .eq(
            'personality_type',
            _currentUser?.personalityPreference ?? 'mere_africaine',
          )
          .eq('is_active', true);

      return (response as List)
          .map((json) => AIPersonalityResponse.fromJson(json))
          .toList();
    } catch (e) {
      debugPrint('Erreur lors de la r√©cup√©ration des r√©ponses: $e');
      return [];
    }
  }

  Future<void> _showNotification(String title, String body) async {
    const androidDetails = AndroidNotificationDetails(
      'hordvoice_channel',
      'HordVoice Notifications',
      channelDescription: 'Notifications de l\'assistant vocal HordVoice',
      importance: Importance.high,
      priority: Priority.high,
    );

    const notificationDetails = NotificationDetails(android: androidDetails);

    await _notifications.show(
      DateTime.now().millisecond,
      title,
      body,
      notificationDetails,
    );
  }

  Future<Map<String, dynamic>> _getSystemStatus() async {
    final batteryLevel = await _battery.batteryLevel;
    final position = await Geolocator.getCurrentPosition();

    return {
      'battery_level': batteryLevel,
      'location': {
        'latitude': position.latitude,
        'longitude': position.longitude,
      },
      'mood': _currentMood,
      'is_listening': _isListening,
      'timestamp': DateTime.now().toIso8601String(),
    };
  }

  Future<void> _checkSystemHealth() async {
    final systemStatus = await _getSystemStatus();
    _systemStatusController.add(systemStatus);
  }

  String get currentMood => _currentMood;
  bool get isListening => _isListening;
  bool get isInitialized => _isInitialized;
  UserProfile? get currentUser => _currentUser;

  // M√©thodes pour l'interface vocale
  Future<void> startVoiceRecognition() async {
    if (!_isInitialized) throw Exception('Service non initialis√©');

    try {
      _isListening = true;

      // Utiliser le nouveau pipeline streaming (√âtape 6)
      await _azureSpeechService.startListening();

      // √âcouter les r√©sultats via streams
      _azureSpeechService.resultStream.listen((result) {
        if (result.recognizedText.isNotEmpty && result.isFinal) {
          processVoiceCommand(result.recognizedText);
        }
      });

      debugPrint('Reconnaissance vocale d√©marr√©e');
    } catch (e) {
      _isListening = false;
      debugPrint('Erreur lors du d√©marrage de la reconnaissance: $e');
      rethrow;
    }
  }

  Future<void> stopVoiceRecognition() async {
    try {
      _isListening = false;
      await _azureSpeechService.stopListening();
      debugPrint('Reconnaissance vocale arr√™t√©e');
    } catch (e) {
      debugPrint('Erreur lors de l\'arr√™t de la reconnaissance: $e');
      rethrow;
    }
  }

  Future<String> processVoiceCommand(String command) async {
    if (!_isInitialized) throw Exception('Service non initialis√©');

    try {
      debugPrint('Traitement de la commande: $command');

      // Analyser l'√©motion de la commande
      final emotion = await _emotionAnalysisService.analyzeEmotion(command);

      // G√©n√©rer une r√©ponse appropri√©e
      final response = await _aiService.generateContextualResponse(
        command,
        'voice_command',
        {'emotion': emotion, 'mood': _currentMood, 'context': 'voice_command'},
      );

      // Parler la r√©ponse
      await speakText(response);

      // Ex√©cuter la commande si n√©cessaire
      await _executeCommand(command);

      return response;
    } catch (e) {
      debugPrint('Erreur lors du traitement de la commande: $e');
      final errorResponse =
          'D√©sol√©, je n\'ai pas pu traiter votre demande. Veuillez r√©essayer.';
      await speakText(errorResponse);
      return errorResponse;
    }
  }

  Future<void> _executeCommand(String command) async {
    final lowerCommand = command.toLowerCase();

    // Contr√¥les vocaux des param√®tres syst√®me
    if (lowerCommand.contains('luminosit√©') || lowerCommand.contains('√©cran')) {
      await _quickSettingsService.handleBrightnessVoiceCommand(command);
    } else if (lowerCommand.contains('volume') ||
        lowerCommand.contains('son')) {
      await _quickSettingsService.handleVolumeVoiceCommand(command);
    } else if (lowerCommand.contains('wifi') ||
        lowerCommand.contains('bluetooth')) {
      await _quickSettingsService.handleConnectivityVoiceCommand(command);
    } else if (lowerCommand.contains('mode avion') ||
        lowerCommand.contains('donn√©es mobiles') ||
        lowerCommand.contains('rotation')) {
      await _quickSettingsService.handlePhoneSettingsVoiceCommand(command);
    } else if (lowerCommand.contains('m√©t√©o')) {
      await _handleWeatherRequest(command);
    } else if (lowerCommand.contains('musique') ||
        lowerCommand.contains('jouer')) {
      await _handleMusicRequest(command);
    } else if (lowerCommand.contains('aller √†') ||
        lowerCommand.contains('navigation')) {
      await _handleNavigationRequest(command);
    } else if (lowerCommand.contains('calendrier') ||
        lowerCommand.contains('√©v√©nement')) {
      await _handleCalendarRequest(command);
    } else if (lowerCommand.contains('sant√©') || lowerCommand.contains('pas')) {
      await _handleHealthRequest(command);
    } else if (lowerCommand.contains('appeler')) {
      await _handleCallRequest(command);
    } else if (lowerCommand.contains('message') ||
        lowerCommand.contains('sms')) {
      await _handleSMSRequest(command);
    }
  }

  Future<void> _handleCallRequest(String command) async {
    try {
      // Extraire le nom ou num√©ro de la commande
      final lowerCommand = command.toLowerCase();
      String contact = '';

      if (lowerCommand.contains('appeler ')) {
        contact = command
            .substring(command.toLowerCase().indexOf('appeler ') + 8)
            .trim();
      }

      if (contact.isNotEmpty) {
        await speakText('Je vais appeler $contact pour vous.');
        // Ici, vous pouvez int√©grer avec l'API d'appels du t√©l√©phone
        debugPrint('Tentative d\'appel √†: $contact');
      } else {
        await speakText('Qui voulez-vous appeler ?');
      }
    } catch (e) {
      debugPrint('Erreur lors de l\'appel: $e');
      await speakText('D√©sol√©, je n\'ai pas pu passer l\'appel.');
    }
  }

  Future<void> _handleSMSRequest(String command) async {
    try {
      // Extraire le destinataire et le message
      final lowerCommand = command.toLowerCase();
      String contact = '';
      String message = '';

      if (lowerCommand.contains('envoyer message √† ')) {
        final parts = command.split('envoyer message √† ');
        if (parts.length > 1) {
          final remaining = parts[1];
          final messageParts = remaining.split(' dire ');
          if (messageParts.length > 1) {
            contact = messageParts[0].trim();
            message = messageParts[1].trim();
          }
        }
      }

      if (contact.isNotEmpty && message.isNotEmpty) {
        await speakText('J\'envoie le message "$message" √† $contact.');
        // Ici, vous pouvez int√©grer avec l'API SMS du t√©l√©phone
        debugPrint('Envoi SMS √† $contact: $message');
      } else {
        await speakText('Veuillez pr√©ciser le destinataire et le message.');
      }
    } catch (e) {
      debugPrint('Erreur lors de l\'envoi SMS: $e');
      await speakText('D√©sol√©, je n\'ai pas pu envoyer le message.');
    }
  }

  /// D√©marre la d√©tection automatique de wake word en arri√®re-plan
  Future<void> _startWakeWordDetection() async {
    if (_wakeWordActive) return;

    try {
      _wakeWordActive = true;
      debugPrint('D√©marrage de la d√©tection automatique de wake word');

      // D√©marrer l'√©coute en continu pour le wake word
      _wakeWordTimer = Timer.periodic(const Duration(seconds: 5), (
        timer,
      ) async {
        if (!_isListening && _wakeWordActive) {
          await _listenForWakeWord();
        }
      });

      // √âcouter imm√©diatement
      await _listenForWakeWord();
    } catch (e) {
      debugPrint('Erreur d√©marrage wake word detection: $e');
    }
  }

  /// √âcoute pour le wake word
  Future<void> _listenForWakeWord() async {
    try {
      // D√©marrer une reconnaissance courte pour d√©tecter "Hey Ric" ou variantes
      final result = await _azureSpeechService.startSimpleRecognition();

      if (result != null && _containsWakeWord(result)) {
        debugPrint('Wake word d√©tect√©: $result');
        _wakeWordController.add(true);

        await speakText('Oui, je vous √©coute');

        // D√©marrer l'√©coute compl√®te pour la commande
        await startListening();
      }
    } catch (e) {
      debugPrint('Erreur √©coute wake word: $e');
    }
  }

  /// V√©rifie si le texte contient un wake word
  bool _containsWakeWord(String text) {
    final lowerText = text.toLowerCase();
    final wakeWords = [
      'hey ric',
      'salut ric',
      'ric',
      'rick',
      'hey rick',
      'salut rick',
      'bonjour ric',
      'bonjour rick',
    ];

    return wakeWords.any((wake) => lowerText.contains(wake));
  }

  /// Arr√™te la d√©tection de wake word
  Future<void> stopWakeWordDetection() async {
    _wakeWordActive = false;
    _wakeWordTimer?.cancel();
    _wakeWordTimer = null;
    debugPrint('D√©tection wake word arr√™t√©e');
  }

  // Acc√®s au service de transition pour les animations
  TransitionAnimationService get transitionService => _transitionService;

  void dispose() {
    _monitoringTimer?.cancel();
    _batteryTimer?.cancel();
    _wakeWordTimer?.cancel();
    _aiResponseController.close();
    _moodController.close();
    _systemStatusController.close();
    _audioLevelController.close();
    _transcriptionController.close();
    _emotionController.close();
    _wakeWordController.close();
    _isSpeakingController.close();
  }
}
