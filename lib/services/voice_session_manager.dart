import 'dart:async';
import 'dart:math';
import 'package:flutter/foundation.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:flutter_tts/flutter_tts.dart';
import 'package:audio_session/audio_session.dart';
import 'azure_speech_service.dart';
import 'azure_openai_service.dart';
import 'permission_manager_service.dart';
import 'emotional_avatar_service.dart';

/// Provider pour le gestionnaire de session vocale
final voiceSessionManagerProvider =
    StateNotifierProvider<VoiceSessionManager, VoiceSessionState>(
      (ref) => VoiceSessionManager(),
    );

/// √âtats possibles du gestionnaire de session vocale
enum VoiceSessionStatus {
  idle, // Inactif - pr√™t √† √©couter
  listening, // En √©coute active (STT)
  processing, // Traitement de la requ√™te (GPT)
  speaking, // Synth√®se vocale en cours (TTS)
  error, // Erreur rencontr√©e
  interrupted, // Interrompu par l'utilisateur
}

/// √âtat complet de la session vocale
class VoiceSessionState {
  final VoiceSessionStatus status;
  final String? currentTranscript;
  final String? currentResponse;
  final String? errorMessage;
  final double confidenceLevel;
  final bool isMuted;
  final bool isListening;
  final bool isSpeaking;
  final List<String> sessionHistory;
  final DateTime? lastActivity;
  final String? currentEmotion;
  final double audioLevel;

  const VoiceSessionState({
    this.status = VoiceSessionStatus.idle,
    this.currentTranscript,
    this.currentResponse,
    this.errorMessage,
    this.confidenceLevel = 0.0,
    this.isMuted = false,
    this.isListening = false,
    this.isSpeaking = false,
    this.sessionHistory = const [],
    this.lastActivity,
    this.currentEmotion,
    this.audioLevel = 0.0,
  });

  VoiceSessionState copyWith({
    VoiceSessionStatus? status,
    String? currentTranscript,
    String? currentResponse,
    String? errorMessage,
    double? confidenceLevel,
    bool? isMuted,
    bool? isListening,
    bool? isSpeaking,
    List<String>? sessionHistory,
    DateTime? lastActivity,
    String? currentEmotion,
    double? audioLevel,
  }) {
    return VoiceSessionState(
      status: status ?? this.status,
      currentTranscript: currentTranscript ?? this.currentTranscript,
      currentResponse: currentResponse ?? this.currentResponse,
      errorMessage: errorMessage ?? this.errorMessage,
      confidenceLevel: confidenceLevel ?? this.confidenceLevel,
      isMuted: isMuted ?? this.isMuted,
      isListening: isListening ?? this.isListening,
      isSpeaking: isSpeaking ?? this.isSpeaking,
      sessionHistory: sessionHistory ?? this.sessionHistory,
      lastActivity: lastActivity ?? this.lastActivity,
      currentEmotion: currentEmotion ?? this.currentEmotion,
      audioLevel: audioLevel ?? this.audioLevel,
    );
  }
}

/// Gestionnaire centralis√© des sessions vocales - √âvite les conflits STT/TTS
class VoiceSessionManager extends StateNotifier<VoiceSessionState> {
  VoiceSessionManager() : super(const VoiceSessionState()) {
    _initialize();
  }

  // Services
  late AzureSpeechService _speechService;
  late AzureOpenAIService _aiService;
  late FlutterTts _tts;
  late AudioSession _audioSession;

  // Service √©motionnel pour r√©activit√© avatar
  EmotionalAvatarService? _emotionalService;

  // Controllers & Streams
  StreamSubscription? _speechSubscription;
  StreamSubscription? _speechErrorSubscription;
  Timer? _sessionTimer;
  Timer? _confidenceTimer;

  // Configuration
  static const Duration _silenceTimeout = Duration(seconds: 5);

  // Flags de contr√¥le - CRITIQUES pour √©viter concurrence
  bool _isInitialized = false;
  bool _isSttActive = false;
  bool _isTtsActive = false;
  bool _isProcessingRequest = false;
  final List<VoidCallback> _pendingActions = [];

  /// Initialisation des services
  Future<void> _initialize() async {
    if (_isInitialized) return;

    try {
      debugPrint('üé§ Initialisation VoiceSessionManager...');

      // V√©rifier les permissions micro
      final hasPermissions =
          await PermissionManagerService.hasEssentialPermissions();

      if (!hasPermissions) {
        _setError('Permission microphone requise');
        return;
      }

      // Initialiser session audio
      await _initializeAudioSession();

      // Initialiser Azure Speech
      _speechService = AzureSpeechService();
      await _speechService.initialize();

      // Initialiser Azure OpenAI
      _aiService = AzureOpenAIService();
      await _aiService.initialize();

      // Initialiser TTS
      await _initializeTts();

      // √âcouter les √©v√©nements de reconnaissance
      _setupSpeechListeners();

      _isInitialized = true;
      state = state.copyWith(
        status: VoiceSessionStatus.idle,
        lastActivity: DateTime.now(),
      );

      debugPrint('‚úÖ VoiceSessionManager initialis√© avec succ√®s');
    } catch (e) {
      debugPrint('‚ùå Erreur initialisation VoiceSessionManager: $e');
      _setError('Erreur initialisation: $e');
    }
  }

  /// Configuration session audio - CRITIQUE pour √©viter conflits
  Future<void> _initializeAudioSession() async {
    _audioSession = await AudioSession.instance;
    await _audioSession.configure(
      AudioSessionConfiguration(
        avAudioSessionCategory: AVAudioSessionCategory.playAndRecord,
        avAudioSessionCategoryOptions:
            AVAudioSessionCategoryOptions.defaultToSpeaker,
        avAudioSessionMode: AVAudioSessionMode.spokenAudio,
        avAudioSessionRouteSharingPolicy:
            AVAudioSessionRouteSharingPolicy.defaultPolicy,
        avAudioSessionSetActiveOptions: AVAudioSessionSetActiveOptions.none,
        androidAudioAttributes: const AndroidAudioAttributes(
          contentType: AndroidAudioContentType.speech,
          flags: AndroidAudioFlags.none,
          usage: AndroidAudioUsage.voiceCommunication,
        ),
        androidAudioFocusGainType: AndroidAudioFocusGainType.gain,
        androidWillPauseWhenDucked: false,
      ),
    );

    debugPrint('üîä Session audio configur√©e');
  }

  /// Initialisation TTS avec callbacks
  Future<void> _initializeTts() async {
    _tts = FlutterTts();

    // Configuration de base
    await _tts.setLanguage('fr-FR');
    await _tts.setSpeechRate(0.8);
    await _tts.setVolume(0.9);
    await _tts.setPitch(1.0);

    // Callbacks critiques pour tracking √©tat
    _tts.setStartHandler(() {
      debugPrint('üó£Ô∏è TTS d√©marr√©');
      _isTtsActive = true;
      state = state.copyWith(
        status: VoiceSessionStatus.speaking,
        isSpeaking: true,
        lastActivity: DateTime.now(),
      );
    });

    _tts.setCompletionHandler(() {
      debugPrint('‚úÖ TTS termin√©');
      _isTtsActive = false;
      _onTtsCompleted();
    });

    _tts.setErrorHandler((message) {
      debugPrint('‚ùå Erreur TTS: $message');
      _isTtsActive = false;
      _handleTtsError(message);
    });

    debugPrint('üó£Ô∏è TTS initialis√©');
  }

  /// Configuration des listeners pour Azure Speech
  void _setupSpeechListeners() {
    _speechSubscription = _speechService.resultStream.listen(
      (result) => _handleSpeechResult(result),
      onError: (error) => _handleSpeechError(error),
    );
  }

  /// **M√âTHODE PUBLIQUE** - Connecter le service √©motionnel pour r√©activit√© avatar
  void connectEmotionalService(EmotionalAvatarService emotionalService) {
    _emotionalService = emotionalService;
    debugPrint('üé≠ Service √©motionnel connect√© au VoiceSessionManager');
  }

  /// **M√âTHODE PUBLIQUE** - D√©marrer l'√©coute (respecte s√©quencement)
  Future<bool> startListening() async {
    if (!_isInitialized) {
      debugPrint('‚ö†Ô∏è VoiceSessionManager non initialis√©');
      return false;
    }

    // V√âRIFICATION CRITIQUE: Pas de TTS en cours
    if (_isTtsActive) {
      debugPrint('‚ö†Ô∏è TTS en cours - √©coute bloqu√©e');
      return false;
    }

    // V√âRIFICATION CRITIQUE: D√©j√† en √©coute
    if (_isSttActive) {
      debugPrint('‚ö†Ô∏è STT d√©j√† actif');
      return false;
    }

    try {
      debugPrint('üé§ D√©marrage √©coute...');

      // Demander audio focus
      await _audioSession.setActive(true);

      // D√©marrer reconnaissance
      await _speechService.startListening();

      _isSttActive = true;
      state = state.copyWith(
        status: VoiceSessionStatus.listening,
        isListening: true,
        currentTranscript: null,
        errorMessage: null,
        lastActivity: DateTime.now(),
      );

      // Timeout de silence
      _startSilenceTimer();

      debugPrint('‚úÖ √âcoute d√©marr√©e');
      return true;
    } catch (e) {
      debugPrint('‚ùå Erreur d√©marrage √©coute: $e');
      _isSttActive = false;
      _setError('Erreur d√©marrage √©coute: $e');
      return false;
    }
  }

  /// **M√âTHODE PUBLIQUE** - Arr√™ter l'√©coute
  Future<void> stopListening() async {
    if (!_isSttActive) return;

    try {
      debugPrint('üõë Arr√™t √©coute...');

      await _speechService.stopListening();
      _isSttActive = false;

      state = state.copyWith(
        status: VoiceSessionStatus.idle,
        isListening: false,
      );

      _cancelSilenceTimer();
      debugPrint('‚úÖ √âcoute arr√™t√©e');
    } catch (e) {
      debugPrint('‚ùå Erreur arr√™t √©coute: $e');
      _isSttActive = false;
    }
  }

  /// **M√âTHODE PUBLIQUE** - Synth√®se vocale (respecte s√©quencement)
  Future<bool> speak(String text, {String? emotion}) async {
    if (!_isInitialized || text.isEmpty) return false;

    // V√âRIFICATION CRITIQUE: Arr√™ter STT d'abord
    if (_isSttActive) {
      debugPrint('üõë Arr√™t STT avant TTS');
      await stopListening();
      await Future.delayed(
        const Duration(milliseconds: 300),
      ); // D√©lai stabilisation
    }

    // V√âRIFICATION CRITIQUE: Pas de TTS concurrent
    if (_isTtsActive) {
      debugPrint('‚ö†Ô∏è TTS d√©j√† actif - arr√™t forc√©');
      await _tts.stop();
      await Future.delayed(const Duration(milliseconds: 200));
    }

    try {
      debugPrint(
        'üó£Ô∏è D√©marrage TTS: ${text.substring(0, min(50, text.length))}...',
      );

      // Configurer √©motion si fournie
      if (emotion != null) {
        await _configureEmotionalTts(emotion);
      }

      // Demander audio focus
      await _audioSession.setActive(true);

      state = state.copyWith(currentResponse: text, currentEmotion: emotion);

      // D√©marrer synth√®se
      await _tts.speak(text);

      return true;
    } catch (e) {
      debugPrint('‚ùå Erreur TTS: $e');
      _isTtsActive = false;
      _setError('Erreur synth√®se vocale: $e');
      return false;
    }
  }

  /// **M√âTHODE PUBLIQUE** - Arr√™ter toute activit√©
  Future<void> stopAll() async {
    debugPrint('üõë Arr√™t complet session vocale');

    try {
      // Arr√™ter TTS imm√©diatement
      if (_isTtsActive) {
        await _tts.stop();
        _isTtsActive = false;
      }

      // Arr√™ter STT
      if (_isSttActive) {
        await _speechService.stopListening();
        _isSttActive = false;
      }

      // Lib√©rer audio focus
      await _audioSession.setActive(false);

      state = state.copyWith(
        status: VoiceSessionStatus.idle,
        isListening: false,
        isSpeaking: false,
        currentTranscript: null,
        currentResponse: null,
      );

      _cancelAllTimers();
      debugPrint('‚úÖ Session arr√™t√©e');
    } catch (e) {
      debugPrint('‚ùå Erreur arr√™t session: $e');
    }
  }

  /// **M√âTHODE PUBLIQUE** - Traitement complet requ√™te vocale
  Future<void> processVoiceRequest(String transcript) async {
    if (_isProcessingRequest) {
      debugPrint('‚ö†Ô∏è Traitement d√©j√† en cours');
      return;
    }

    _isProcessingRequest = true;

    try {
      debugPrint('üß† Traitement requ√™te: $transcript');

      state = state.copyWith(
        status: VoiceSessionStatus.processing,
        currentTranscript: transcript,
      );

      // Analyser intention et extraire slots
      final response = await _aiService.generatePersonalizedResponse(
        transcript,
        'voice_assistant',
        'user_${DateTime.now().millisecondsSinceEpoch}',
        state.sessionHistory.take(5).toList(),
      );

      if (response.isNotEmpty) {
        // D√©marrer TTS avec la r√©ponse
        await speak(response, emotion: 'neutral');

        // Ajouter √† l'historique
        final newHistory = [
          ...state.sessionHistory,
          'U: $transcript',
          'A: $response',
        ];
        state = state.copyWith(sessionHistory: newHistory);
      } else {
        await speak(
          'Je n\'ai pas bien compris votre demande. Pouvez-vous r√©p√©ter ?',
          emotion: 'apologetic',
        );
      }
    } catch (e) {
      debugPrint('‚ùå Erreur traitement requ√™te: $e');
      await speak(
        'D√©sol√©, une erreur s\'est produite. Veuillez r√©essayer.',
        emotion: 'apologetic',
      );
    } finally {
      _isProcessingRequest = false;
    }
  }

  /// Gestion r√©sultat reconnaissance vocale
  void _handleSpeechResult(SpeechRecognitionResult result) {
    final text = result.recognizedText;
    final isInterim = !result.isFinal;
    final confidence = result.confidence;

    if (text.isNotEmpty) {
      state = state.copyWith(
        currentTranscript: text,
        confidenceLevel: confidence,
        lastActivity: DateTime.now(),
      );

      // R√©action √©motionnelle √† la voix d√©tect√©e
      if (_emotionalService != null && !isInterim) {
        _emotionalService!.onVoiceStimulus(
          volume: confidence, // Utiliser la confiance comme proxy du volume
          pitch:
              200, // Pitch par d√©faut (pourrait √™tre am√©lior√© avec analyse audio)
          emotion: _detectVoiceEmotion(text),
          content: text,
        );
      }

      // Si r√©sultat final avec bonne confiance
      if (!isInterim && confidence > 0.7) {
        debugPrint(
          '‚úÖ Transcription finale: $text (conf: ${(confidence * 100).toInt()}%)',
        );

        // Arr√™ter l'√©coute et traiter
        stopListening().then((_) {
          processVoiceRequest(text);
        });
      }
    }
  }

  /// Gestion erreurs reconnaissance
  void _handleSpeechError(dynamic error) {
    debugPrint('‚ùå Erreur STT: $error');
    _isSttActive = false;
    _setError('Erreur reconnaissance: $error');
  }

  /// Callback fin TTS
  void _onTtsCompleted() {
    state = state.copyWith(
      status: VoiceSessionStatus.idle,
      isSpeaking: false,
      lastActivity: DateTime.now(),
    );

    // Traiter actions en attente
    _processPendingActions();
  }

  /// Gestion erreur TTS
  void _handleTtsError(String message) {
    _setError('Erreur TTS: $message');
    _onTtsCompleted(); // Reset √©tat
  }

  /// Configuration TTS √©motionnel
  Future<void> _configureEmotionalTts(String emotion) async {
    switch (emotion.toLowerCase()) {
      case 'joy':
      case 'joie':
        await _tts.setPitch(1.2);
        await _tts.setSpeechRate(0.9);
        break;
      case 'calm':
      case 'calme':
        await _tts.setPitch(0.9);
        await _tts.setSpeechRate(0.7);
        break;
      case 'apologetic':
      case 'd√©sol√©':
        await _tts.setPitch(0.8);
        await _tts.setSpeechRate(0.6);
        break;
      default:
        await _tts.setPitch(1.0);
        await _tts.setSpeechRate(0.8);
    }
  }

  /// Timer silence - arr√™te √©coute si pas d'activit√©
  void _startSilenceTimer() {
    _cancelSilenceTimer();
    _sessionTimer = Timer(_silenceTimeout, () {
      if (_isSttActive && state.currentTranscript?.isEmpty != false) {
        debugPrint('‚è∞ Timeout silence - arr√™t √©coute');
        stopListening();
      }
    });
  }

  void _cancelSilenceTimer() {
    _sessionTimer?.cancel();
    _sessionTimer = null;
  }

  void _cancelAllTimers() {
    _cancelSilenceTimer();
    _confidenceTimer?.cancel();
    _confidenceTimer = null;
  }

  /// Gestion actions en attente
  void _processPendingActions() {
    if (_pendingActions.isNotEmpty) {
      final action = _pendingActions.removeAt(0);
      action();
    }
  }

  /// Set erreur avec cleanup
  void _setError(String message) {
    debugPrint('‚ùå VoiceSessionManager Error: $message');
    state = state.copyWith(
      status: VoiceSessionStatus.error,
      errorMessage: message,
      isListening: false,
      isSpeaking: false,
    );

    // Cleanup
    _isSttActive = false;
    _isTtsActive = false;
    _cancelAllTimers();
  }

  /// D√©tecte l'√©motion bas√©e sur le contenu vocal (simple analyse de mots-cl√©s)
  String? _detectVoiceEmotion(String text) {
    final lowerText = text.toLowerCase();

    // Mots joyeux
    if (lowerText.contains(
      RegExp(
        r'\b(super|g√©nial|fantastique|excellent|bravo|merci|content|heureux|joie)\b',
      ),
    )) {
      return 'happy';
    }

    // Mots excit√©s
    if (lowerText.contains(
      RegExp(r'\b(wow|incroyable|amazing|parfait|formidable|magnifique)\b'),
    )) {
      return 'excited';
    }

    // Mots tristes
    if (lowerText.contains(
      RegExp(r'\b(triste|d√©prim√©|mal|probl√®me|erreur|dommage)\b'),
    )) {
      return 'sad';
    }

    // Mots de surprise
    if (lowerText.contains(
      RegExp(r'\b(quoi|vraiment|surprise|√©tonnant|inattendu)\b'),
    )) {
      return 'surprised';
    }

    // Questions (confusion potentielle)
    if (lowerText.contains(RegExp(r'\b(comment|pourquoi|que|quoi)\b')) ||
        lowerText.contains('?')) {
      return 'confused';
    }

    return null; // Neutre par d√©faut
  }

  /// Cleanup √† la destruction
  @override
  void dispose() {
    debugPrint('üßπ Cleanup VoiceSessionManager');

    stopAll();
    _speechSubscription?.cancel();
    _cancelAllTimers();

    super.dispose();
  }

  /// M√©thodes utilitaires publiques
  bool get isReady => _isInitialized && state.status == VoiceSessionStatus.idle;
  bool get isBusy => _isSttActive || _isTtsActive || _isProcessingRequest;
  bool get canStartListening => isReady && !_isTtsActive;
}
